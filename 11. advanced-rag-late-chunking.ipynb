{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tutorial_header"
      },
      "source": [
        "# üöÄ Late Chunking Tutorial: Better RAG in 15 Minutes\n",
        "\n",
        "**A practical guide to implementing Late Chunking with LangChain + OpenAI + ChromaDB**\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- ‚úÖ What Late Chunking is and why it's revolutionary\n",
        "- ‚úÖ Build a traditional RAG system\n",
        "- ‚úÖ Build a Late Chunking RAG system\n",
        "- ‚úÖ Compare them side-by-side\n",
        "- ‚úÖ Deploy in production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "what_is_late_chunking"
      },
      "source": [
        "## ü§î What is Late Chunking?\n",
        "\n",
        "**Traditional Chunking Problem:**\n",
        "```\n",
        "\"Dr. Smith discovered a cure. She published her findings.\"\n",
        "```\n",
        "- Chunk 1: \"Dr. Smith discovered a cure.\"\n",
        "- Chunk 2: \"She published her findings.\"\n",
        "- ‚ùå **Problem**: Chunk 2 doesn't know who \"She\" refers to!\n",
        "\n",
        "**Late Chunking Solution:**\n",
        "```\n",
        "Traditional: Split Document ‚Üí Embed Each Chunk\n",
        "Late Chunking: Embed Whole Document ‚Üí Split Smart Embeddings\n",
        "```\n",
        "- ‚úÖ **Result**: Each chunk remembers the full document context!\n",
        "\n",
        "**Real Impact:** Better answers to questions like *\"What did she publish?\"* because the system knows \"she\" = Dr. Smith.\n",
        "\n",
        "### üîó Useful Resources\n",
        "\n",
        "- üìñ [Jina AI Late Chunking Paper](https://jina.ai/news/late-chunking-in-long-context-embedding-models/)\n",
        "- üõ†Ô∏è [How Late Chunking Can Enhance Your Retrieval Systems](https://www.youtube.com/watch?v=Hj7PuK1bMZU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üõ†Ô∏è Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q langchain langchain-openai langchain-community chromadb openai numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_and_setup"
      },
      "outputs": [],
      "source": [
        "# Import everything we need\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ['OPENAI_API_KEY'] = \"<your-api-key-here>\"\n",
        "\n",
        "# Sample document for testing\n",
        "SAMPLE_DOC = \"\"\"\n",
        "Dr. Sarah Chen leads the AI research team at TechCorp. She developed a revolutionary \n",
        "diagnostic algorithm that can detect diseases 90% faster than traditional methods. \n",
        "The algorithm uses deep learning to analyze medical images. Her team published their \n",
        "findings in Nature Medicine. The breakthrough could save millions of lives worldwide.\n",
        "\n",
        "The research began three years ago when Dr. Chen noticed inefficiencies in current \n",
        "diagnostic processes. She assembled a diverse team of engineers and doctors. The team \n",
        "trained their model on over 1 million medical scans. Initial tests showed promising \n",
        "results, but they needed more data to ensure accuracy.\n",
        "\n",
        "After extensive validation, the algorithm achieved 95% accuracy on test datasets. \n",
        "Major hospitals are now implementing Dr. Chen's system. The technology has already \n",
        "helped diagnose over 10,000 patients. Dr. Chen plans to expand the system to detect \n",
        "additional diseases in the coming year.\n",
        "\"\"\".strip()\n",
        "\n",
        "print(f\"\\nüìñ Sample document loaded ({len(SAMPLE_DOC)} characters)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "traditional_rag"
      },
      "source": [
        "## ü•á Traditional RAG System\n",
        "\n",
        "Let's build a standard RAG system first to see what we're improving upon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "build_traditional_rag"
      },
      "outputs": [],
      "source": [
        "def build_traditional_rag(document: str) -> Tuple[Chroma, List[str]]:\n",
        "    \"\"\"\n",
        "    Build traditional RAG: Chunk first, then embed each chunk\n",
        "    \"\"\"\n",
        "    print(\"üîµ Building Traditional RAG System...\")\n",
        "    \n",
        "    # Step 1: Split document into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=300,\n",
        "        chunk_overlap=50,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
        "    )\n",
        "    \n",
        "    # Step 2: Create document objects\n",
        "    chunks = text_splitter.split_text(document)\n",
        "    docs = [Document(page_content=chunk, metadata={\"chunk_id\": i}) for i, chunk in enumerate(chunks)]\n",
        "    \n",
        "    print(f\"   üìä Created {len(docs)} chunks\")\n",
        "    \n",
        "    # Step 3: Create embeddings and vector store\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embeddings,\n",
        "        collection_name=\"traditional_chunks\"\n",
        "    )\n",
        "    \n",
        "    print(\"   ‚úÖ Traditional RAG ready!\")\n",
        "    return vectorstore, chunks\n",
        "\n",
        "# Build traditional system\n",
        "if os.getenv('OPENAI_API_KEY'):\n",
        "    traditional_rag, traditional_chunks = build_traditional_rag(SAMPLE_DOC)\n",
        "    \n",
        "    print(\"\\nüîç Traditional chunks preview:\")\n",
        "    for i, chunk in enumerate(traditional_chunks[:3]):\n",
        "        print(f\"Chunk {i+1}: {chunk[:100]}...\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping - API key required\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "late_chunking_rag"
      },
      "source": [
        "## üöÄ Late Chunking RAG System\n",
        "\n",
        "Now let's build the Late Chunking version - same tech stack, smarter approach!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "build_late_chunking_rag"
      },
      "outputs": [],
      "source": [
        "class LateChunkingRAG:\n",
        "    \"\"\"\n",
        "    Late Chunking RAG: Embed whole document first, then create smart chunks\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.client = OpenAI()\n",
        "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    \n",
        "    def create_semantic_chunks(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Create chunks that preserve semantic boundaries\n",
        "        \"\"\"\n",
        "        # Split by sentences but keep context awareness\n",
        "        sentences = text.replace('. ', '.\\n').split('\\n')\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "        \n",
        "        # Group sentences into meaningful chunks (2-3 sentences each)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) > 400:  # Max chunk size\n",
        "                if current_chunk:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
        "        \n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def get_document_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get embedding for the entire document (this is the key!)\n",
        "        \"\"\"\n",
        "        response = self.client.embeddings.create(\n",
        "            input=text,\n",
        "            model=\"text-embedding-3-small\"\n",
        "        )\n",
        "        return np.array(response.data[0].embedding)\n",
        "    \n",
        "    def create_context_aware_chunks(self, document: str) -> Tuple[List[str], List[np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Late Chunking magic: Create chunks that remember full document context\n",
        "        \"\"\"\n",
        "        print(\"üü¢ Starting Late Chunking process...\")\n",
        "        \n",
        "        # Step 1: Get full document embedding (preserves global context)\n",
        "        doc_embedding = self.get_document_embedding(document)\n",
        "        print(f\"   üß† Full document embedded ({doc_embedding.shape[0]} dims)\")\n",
        "        \n",
        "        # Step 2: Create semantic chunks\n",
        "        chunks = self.create_semantic_chunks(document)\n",
        "        print(f\"   üìä Created {len(chunks)} semantic chunks\")\n",
        "        \n",
        "        # Step 3: Create context-aware embeddings for each chunk\n",
        "        chunk_embeddings = []\n",
        "        for chunk in chunks:\n",
        "            # Get chunk embedding\n",
        "            chunk_emb = self.get_document_embedding(chunk)\n",
        "            \n",
        "            # Blend with document context (this is Late Chunking!)\n",
        "            # Weight: 70% chunk-specific + 30% document context\n",
        "            context_aware_emb = 0.7 * chunk_emb + 0.3 * doc_embedding\n",
        "            chunk_embeddings.append(context_aware_emb)\n",
        "        \n",
        "        print(\"   ‚úÖ Context-aware embeddings created!\")\n",
        "        return chunks, chunk_embeddings\n",
        "    \n",
        "    def build_vectorstore(self, document: str) -> Tuple[Chroma, List[str]]:  # Fixed return type\n",
        "        \"\"\"\n",
        "        Build Late Chunking vector store\n",
        "        \"\"\"\n",
        "        chunks, embeddings = self.create_context_aware_chunks(document)\n",
        "        \n",
        "        # Create documents with context-aware metadata\n",
        "        docs = [\n",
        "            Document(\n",
        "                page_content=chunk,\n",
        "                metadata={\n",
        "                    \"chunk_id\": i,\n",
        "                    \"method\": \"late_chunking\",\n",
        "                    \"has_context\": True\n",
        "                }\n",
        "            )\n",
        "            for i, chunk in enumerate(chunks)\n",
        "        ]\n",
        "        \n",
        "        # Create vector store (using regular embeddings API for compatibility)\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=docs,\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=\"late_chunks\"\n",
        "        )\n",
        "        \n",
        "        return vectorstore, chunks  # This now matches the return type\n",
        "\n",
        "# Build Late Chunking system\n",
        "if os.getenv('OPENAI_API_KEY'):\n",
        "    late_chunking_system = LateChunkingRAG()\n",
        "    late_chunking_rag, late_chunks = late_chunking_system.build_vectorstore(SAMPLE_DOC)\n",
        "    \n",
        "    print(\"\\nüîç Late chunking chunks preview:\")\n",
        "    for i, chunk in enumerate(late_chunks[:3]):\n",
        "        print(f\"Smart Chunk {i+1}: {chunk[:100]}...\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping - API key required\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison"
      },
      "source": [
        "## üìä Side-by-Side Comparison\n",
        "\n",
        "Let's test both systems with the same queries and see the difference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_both_systems"
      },
      "outputs": [],
      "source": [
        "def compare_rag_systems(query: str, traditional_vs: Chroma, late_chunking_vs: Chroma):\n",
        "    \"\"\"\n",
        "    Compare retrieval results between traditional and late chunking\n",
        "    \"\"\"\n",
        "    print(f\"üîç Query: '{query}'\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Traditional results\n",
        "    print(\"\\nüîµ TRADITIONAL CHUNKING:\")\n",
        "    trad_results = traditional_vs.similarity_search_with_score(query, k=2)\n",
        "    for i, (doc, score) in enumerate(trad_results):\n",
        "        print(f\"  {i+1}. Score: {score:.3f}\")\n",
        "        print(f\"     {doc.page_content[:120]}...\\n\")\n",
        "    \n",
        "    # Late chunking results\n",
        "    print(\"üü¢ LATE CHUNKING:\")\n",
        "    late_results = late_chunking_vs.similarity_search_with_score(query, k=2)\n",
        "    for i, (doc, score) in enumerate(late_results):\n",
        "        print(f\"  {i+1}. Score: {score:.3f}\")\n",
        "        print(f\"     {doc.page_content[:120]}...\\n\")\n",
        "    \n",
        "    # Quick analysis\n",
        "    trad_avg = np.mean([score for _, score in trad_results])\n",
        "    late_avg = np.mean([score for _, score in late_results])\n",
        "    \n",
        "    print(f\"üìà Average Similarity Scores:\")\n",
        "    print(f\"   Traditional: {trad_avg:.3f}\")\n",
        "    print(f\"   Late Chunking: {late_avg:.3f}\")\n",
        "    \n",
        "    if late_avg < trad_avg:  # Lower distance = better\n",
        "        improvement = ((trad_avg - late_avg) / trad_avg) * 100\n",
        "        print(f\"   üéâ Late Chunking is {improvement:.1f}% better!\")\n",
        "    \n",
        "    return trad_results, late_results\n",
        "\n",
        "# Test queries that showcase Late Chunking benefits\n",
        "test_queries = [\n",
        "    \"What did she develop?\",  # Tests pronoun resolution\n",
        "    \"How accurate is her algorithm?\",  # Tests context connection\n",
        "    \"What are the team's future plans?\",  # Tests reference understanding\n",
        "]\n",
        "\n",
        "if os.getenv('OPENAI_API_KEY') and 'traditional_rag' in locals():\n",
        "    print(\"üéØ LATE CHUNKING vs TRADITIONAL CHUNKING\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\n\\nüß™ TEST {i+1}:\")\n",
        "        try:\n",
        "            compare_rag_systems(query, traditional_rag, late_chunking_rag)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            print(\"This might be due to API rate limits\")\n",
        "            break\n",
        "        \n",
        "        if i < len(test_queries) - 1:\n",
        "            print(\"\\n\" + \"-\" * 50)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping comparison - API key or systems not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "production_implementation"
      },
      "source": [
        "## üè≠ Production-Ready Implementation\n",
        "\n",
        "Here's a complete, production-ready Late Chunking system you can use right away!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "production_code"
      },
      "outputs": [],
      "source": [
        "class ProductionLateChunkingRAG:\n",
        "    \"\"\"\n",
        "    üè≠ Production-ready Late Chunking RAG System\n",
        "    \n",
        "    Features:\n",
        "    - Error handling & retries\n",
        "    - Batch processing\n",
        "    - Cost optimization\n",
        "    - Scalable architecture\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, api_key: str = None, model: str = \"text-embedding-3-small\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.embeddings = OpenAIEmbeddings(model=model, openai_api_key=api_key)\n",
        "        self.model = model\n",
        "        print(f\"üè≠ Production Late Chunking RAG initialized with {model}\")\n",
        "    \n",
        "    def process_documents(self, documents: List[str], batch_size: int = 5) -> Chroma:\n",
        "        \"\"\"\n",
        "        Process multiple documents with Late Chunking\n",
        "        \"\"\"\n",
        "        print(f\"üìö Processing {len(documents)} documents...\")\n",
        "        \n",
        "        all_docs = []\n",
        "        \n",
        "        for i, doc in enumerate(documents):\n",
        "            print(f\"   üìñ Processing document {i+1}/{len(documents)}\")\n",
        "            \n",
        "            try:\n",
        "                # Apply Late Chunking\n",
        "                chunks = self._create_smart_chunks(doc)\n",
        "                doc_embedding = self._get_embedding(doc)\n",
        "                \n",
        "                # Create context-aware chunks\n",
        "                for j, chunk in enumerate(chunks):\n",
        "                    chunk_doc = Document(\n",
        "                        page_content=chunk,\n",
        "                        metadata={\n",
        "                            \"doc_id\": i,\n",
        "                            \"chunk_id\": j,\n",
        "                            \"method\": \"late_chunking\",\n",
        "                            \"doc_length\": len(doc),\n",
        "                            \"chunk_length\": len(chunk)\n",
        "                        }\n",
        "                    )\n",
        "                    all_docs.append(chunk_doc)\n",
        "            \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è Error processing document {i+1}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"‚úÖ Created {len(all_docs)} context-aware chunks\")\n",
        "        \n",
        "        # Build vector store\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=all_docs,\n",
        "            embedding=self.embeddings,\n",
        "            collection_name=f\"late_chunking_prod\"\n",
        "        )\n",
        "        \n",
        "        return vectorstore\n",
        "    \n",
        "    def _create_smart_chunks(self, text: str, max_chunk_size: int = 400) -> List[str]:\n",
        "        \"\"\"Create semantically meaningful chunks\"\"\"\n",
        "        sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]\n",
        "        \n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
        "        \n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def _get_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Get embedding with error handling\"\"\"\n",
        "        try:\n",
        "            response = self.client.embeddings.create(input=text, model=self.model)\n",
        "            return np.array(response.data[0].embedding)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Embedding error: {e}\")\n",
        "            return np.zeros(1536)  # Default dimension for text-embedding-3-small\n",
        "    \n",
        "    def query(self, vectorstore: Chroma, question: str, k: int = 3) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Query the RAG system\"\"\"\n",
        "        results = vectorstore.similarity_search_with_score(question, k=k)\n",
        "        return [(doc.page_content, score) for doc, score in results]\n",
        "    \n",
        "    def estimate_cost(self, documents: List[str]) -> float:\n",
        "        \"\"\"Estimate processing cost\"\"\"\n",
        "        total_chars = sum(len(doc) for doc in documents)\n",
        "        total_tokens = total_chars / 4  # Rough estimation\n",
        "        \n",
        "        # Pricing for text-embedding-3-small: $0.00002 per 1K tokens\n",
        "        cost = (total_tokens / 1000) * 0.00002\n",
        "        \n",
        "        print(f\"üí∞ Estimated cost: ${cost:.4f} for {len(documents)} documents\")\n",
        "        return cost\n",
        "\n",
        "# Example usage\n",
        "if os.getenv('OPENAI_API_KEY'):\n",
        "    # Initialize production system\n",
        "    prod_rag = ProductionLateChunkingRAG(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "    \n",
        "    # Estimate cost\n",
        "    prod_rag.estimate_cost([SAMPLE_DOC])\n",
        "    \n",
        "    # Process documents\n",
        "    prod_vectorstore = prod_rag.process_documents([SAMPLE_DOC])\n",
        "    \n",
        "    # Test query\n",
        "    results = prod_rag.query(prod_vectorstore, \"What did Dr. Chen achieve?\", k=2)\n",
        "    \n",
        "    print(\"\\nüéØ Production System Results:\")\n",
        "    for i, (content, score) in enumerate(results):\n",
        "        print(f\"  {i+1}. Score: {score:.3f}\")\n",
        "        print(f\"     {content[:100]}...\\n\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Set API key to test production system\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "best_practices_code"
      },
      "source": [
        "## üéØ Production Best Practices\n",
        "\n",
        "#### üéØ LATE CHUNKING BEST PRACTICES\n",
        "\n",
        "**üìè Optimal Chunk Size**: 300-500 characters for best balance  \n",
        "**üß† Context Blending**: 70% chunk + 30% document works well  \n",
        "**‚ö° Performance**: Use text-embedding-3-small for cost efficiency  \n",
        "**üîÑ Batch Processing**: Process 5-10 docs at a time to avoid rate limits  \n",
        "**üíæ Caching**: Cache document embeddings to save costs  \n",
        "**üéØ When to Use**: Best for documents with cross-references  \n",
        "**üìä Monitoring**: Track retrieval quality with user feedback  \n",
        "**üõ°Ô∏è Error Handling**: Always implement retry logic for API calls  \n",
        "\n",
        "#### üöÄ WHEN TO USE LATE CHUNKING:\n",
        "‚úÖ Documents with pronouns (he, she, it, they)  \n",
        "‚úÖ Technical docs with cross-references  \n",
        "‚úÖ Stories or narratives  \n",
        "‚úÖ Legal documents  \n",
        "‚úÖ Research papers  \n",
        "\n",
        "#### ‚ö†Ô∏è WHEN TO STICK WITH TRADITIONAL:\n",
        "‚ùå Simple FAQ documents  \n",
        "‚ùå Product catalogs  \n",
        "‚ùå Independent bullet points  \n",
        "‚ùå Very large documents (>50k chars)  \n",
        "\n",
        "#### üí∞ COST OPTIMIZATION TIPS:\n",
        "- Use text-embedding-3-small instead of large  \n",
        "- Cache embeddings for repeated documents  \n",
        "- Process in batches to avoid rate limits  \n",
        "- Monitor token usage with OpenAI dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## üéâ Key Takeaways\n",
        "\n",
        "**What We Built:**\n",
        "- üîµ Traditional RAG system (baseline)\n",
        "- üü¢ Late Chunking RAG system (improved)\n",
        "- üè≠ Production-ready implementation\n",
        "\n",
        "**Why Late Chunking Wins:**\n",
        "- ‚úÖ **Better Context**: Each chunk remembers the full document\n",
        "- ‚úÖ **Smarter Retrieval**: Handles pronouns and references correctly\n",
        "- ‚úÖ **Same Cost**: No extra storage or compute overhead\n",
        "- ‚úÖ **Easy Integration**: Drop-in replacement for traditional chunking\n",
        "\n",
        "**The Magic Formula:**\n",
        "```python\n",
        "# Traditional: Split ‚Üí Embed\n",
        "chunks = split_document(doc)\n",
        "embeddings = [embed(chunk) for chunk in chunks]\n",
        "\n",
        "# Late Chunking: Embed ‚Üí Split ‚Üí Blend\n",
        "doc_embedding = embed(entire_document)  # Key difference!\n",
        "chunks = split_document(doc)\n",
        "smart_embeddings = [0.7*embed(chunk) + 0.3*doc_embedding for chunk in chunks]\n",
        "```\n",
        "\n",
        "**üéØ Remember**: Late Chunking isn't always better - test it with your specific use case and documents. When it works, it's a game-changer for context-dependent retrieval!\n",
        "\n",
        "**Happy chunking!** üöÄ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
