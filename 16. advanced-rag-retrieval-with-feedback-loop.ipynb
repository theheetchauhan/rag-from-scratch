{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System with Feedback Loop: Enhancing Retrieval and Response Quality\n",
        "\n",
        "## Overview\n",
        "\n",
        "This system implements a Retrieval-Augmented Generation (RAG) approach with an integrated feedback loop. It aims to improve the quality and relevance of responses over time by incorporating user feedback and dynamically adjusting the retrieval process.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Traditional RAG systems can sometimes produce inconsistent or irrelevant responses due to limitations in the retrieval process or the underlying knowledge base. By implementing a feedback loop, we can:\n",
        "\n",
        "1. Continuously improve the quality of retrieved documents\n",
        "2. Enhance the relevance of generated responses\n",
        "3. Adapt the system to user preferences and needs over time\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **PDF Content Extraction**: Extracts text from PDF documents\n",
        "2. **Vectorstore**: Stores and indexes document embeddings for efficient retrieval\n",
        "3. **Retriever**: Fetches relevant documents based on user queries\n",
        "4. **Language Model**: Generates responses using retrieved documents\n",
        "5. **Feedback Collection**: Gathers user feedback on response quality and relevance\n",
        "6. **Feedback Storage**: Persists user feedback for future use\n",
        "7. **Relevance Score Adjustment**: Modifies document relevance based on feedback\n",
        "8. **Index Fine-tuning**: Periodically updates the vectorstore using accumulated feedback\n",
        "\n",
        "![alt Feedback Retrieval Loop](<feedback loop.png>)\n",
        "\n",
        "## Method Details\n",
        "\n",
        "### 1. Initial Setup\n",
        "- The system reads PDF content and creates a vectorstore\n",
        "- A retriever is initialized using the vectorstore\n",
        "- A language model (LLM) is set up for response generation\n",
        "\n",
        "### 2. Query Processing\n",
        "- When a user submits a query, the retriever fetches relevant documents\n",
        "- The LLM generates a response based on the retrieved documents\n",
        "\n",
        "### 3. Feedback Collection\n",
        "- The system collects user feedback on the response's relevance and quality\n",
        "- Feedback is stored in a JSON file for persistence\n",
        "\n",
        "### 4. Relevance Score Adjustment\n",
        "- For subsequent queries, the system loads previous feedback\n",
        "- An LLM evaluates the relevance of past feedback to the current query\n",
        "- Document relevance scores are adjusted based on this evaluation\n",
        "\n",
        "### 5. Retriever Update\n",
        "- The retriever is updated with the adjusted document scores\n",
        "- This ensures that future retrievals benefit from past feedback\n",
        "\n",
        "### 6. Periodic Index Fine-tuning\n",
        "- At regular intervals, the system fine-tunes the index\n",
        "- High-quality feedback is used to create additional documents\n",
        "- The vectorstore is updated with these new documents, improving overall retrieval quality\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. **Continuous Improvement**: The system learns from each interaction, gradually enhancing its performance.\n",
        "2. **Personalization**: By incorporating user feedback, the system can adapt to individual or group preferences over time.\n",
        "3. **Increased Relevance**: The feedback loop helps prioritize more relevant documents in future retrievals.\n",
        "4. **Quality Control**: Low-quality or irrelevant responses are less likely to be repeated as the system evolves.\n",
        "5. **Adaptability**: The system can adjust to changes in user needs or document contents over time.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This RAG system with a feedback loop represents a significant advancement over traditional RAG implementations. By continuously learning from user interactions, it offers a more dynamic, adaptive, and user-centric approach to information retrieval and response generation. This system is particularly valuable in domains where information accuracy and relevance are critical, and where user needs may evolve over time.\n",
        "\n",
        "While the implementation adds complexity compared to a basic RAG system, the benefits in terms of response quality and user satisfaction make it a worthwhile investment for applications requiring high-quality, context-aware information retrieval and generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for the RAG system\n",
        "# langchain: Framework for building LLM applications\n",
        "# langchain-openai: OpenAI integration for LangChain\n",
        "# langchain-community: Community components for LangChain\n",
        "# pypdf: Library for reading PDF files\n",
        "# chromadb: Vector database for storing embeddings\n",
        "# openai: OpenAI API client\n",
        "!pip install langchain langchain-openai langchain-community pypdf chromadb openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Optional: Set up LangSmith tracking to monitor your LangChain applications\n",
        "# This helps you see what's happening behind the scenes in your RAG system\n",
        "os.environ['LANGSMITH_TRACING'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = '<your-langsmith-api-key>'  # Replace with your actual API key\n",
        "\n",
        "# Set your OpenAI API key here - this is required for embeddings and LLM calls\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<your-api-key>\"  # Replace with your actual OpenAI API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries for the RAG system\n",
        "from langchain.prompts import PromptTemplate  # For creating structured prompts\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # OpenAI models for embeddings and chat\n",
        "from pydantic import BaseModel, Field  # For data validation and structure\n",
        "from langchain.document_loaders import PyPDFLoader  # For loading PDF documents\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting large texts into chunks\n",
        "from langchain.vectorstores import Chroma  # Vector database for storing document embeddings\n",
        "from langchain.chains import RetrievalQA  # Pre-built chain for question-answering with retrieval\n",
        "from langchain.schema import Document  # Document class for LangChain\n",
        "import json  # For handling JSON data (feedback storage)\n",
        "from typing import List, Dict, Any  # Type hints for better code documentation\n",
        "\n",
        "# Define the path where your PDF files are stored\n",
        "# Update this path to match your local directory structure\n",
        "source_path = r\"<you-base-path>\"\n",
        "\n",
        "# Create a list of PDF loaders - each loader handles one PDF file\n",
        "# These are machine learning lecture PDFs that will serve as our knowledge base\n",
        "loaders = [\n",
        "    PyPDFLoader(os.path.join(source_path, \"MachineLearning-Lecture01.pdf\")),\n",
        "    PyPDFLoader(os.path.join(source_path, \"MachineLearning-Lecture02.pdf\")),\n",
        "    PyPDFLoader(os.path.join(source_path, \"MachineLearning-Lecture03.pdf\"))\n",
        "]\n",
        "\n",
        "# Initialize an empty list to store all document pages\n",
        "docs = []\n",
        "\n",
        "# Load each PDF and extract all pages\n",
        "for i, loader in enumerate(loaders):\n",
        "    print(f\"Loading PDF {i+1}/{len(loaders)}...\")  # Progress indicator\n",
        "    # Each PDF might have multiple pages, so we extend (not append) the list\n",
        "    # loader.load() returns a list of Document objects, one per page\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "# Create a text splitter to break large documents into smaller, manageable chunks\n",
        "# This is important because LLMs have token limits and smaller chunks improve retrieval accuracy\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,      # Each chunk will be approximately 1500 characters long\n",
        "    chunk_overlap=150     # 150 characters overlap between chunks to maintain context\n",
        ")\n",
        "\n",
        "# Split all documents into chunks\n",
        "# This converts our list of full documents into a list of smaller text chunks\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Initialize the OpenAI embeddings model\n",
        "# This will convert text chunks into numerical vectors for similarity search\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# Define where to save the vector database locally\n",
        "persist_directory = 'docs/chroma/'\n",
        "\n",
        "# Create a Chroma vector database from our document chunks\n",
        "# This stores the text chunks along with their vector embeddings\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,              # Our document chunks\n",
        "    embedding=embedding,           # The embedding model to convert text to vectors\n",
        "    persist_directory=persist_directory  # Where to save the database locally\n",
        ")\n",
        "\n",
        "# Create a retriever from the vectorstore\n",
        "# This will be used to find relevant documents for user queries\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Initialize the language model for generating responses\n",
        "# Using GPT-4o-mini for cost efficiency while maintaining good performance\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=4000, temperature=0)  # type: ignore\n",
        "\n",
        "# Create a Question-Answering chain that combines retrieval and generation\n",
        "# This chain will: 1) retrieve relevant docs, 2) generate answer using those docs\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
        "\n",
        "print(f\"Successfully loaded {len(docs)} documents and created {len(splits)} chunks\")\n",
        "print(f\"Vector database created with {vectorstore._collection.count()} embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feedback Collection Functions\n",
        "\n",
        "These functions handle collecting, storing, and loading user feedback about the RAG system's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
        "    \"\"\"\n",
        "    Create a structured feedback dictionary to store user evaluations.\n",
        "    \n",
        "    Args:\n",
        "        query (str): The original user question\n",
        "        response (str): The system's generated answer\n",
        "        relevance (int): User rating of how relevant the answer was (1-5 scale)\n",
        "        quality (int): User rating of answer quality (1-5 scale)\n",
        "        comments (str): Optional additional feedback comments\n",
        "    \n",
        "    Returns:\n",
        "        dict: Structured feedback data ready for storage\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"query\": query,                    # Store the original question\n",
        "        \"response\": response,              # Store the system's answer\n",
        "        \"relevance\": int(relevance),       # Convert to integer for consistency\n",
        "        \"quality\": int(quality),           # Convert to integer for consistency\n",
        "        \"comments\": comments               # Store any additional feedback\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_feedback(feedback, file_path=\"data/feedback_data.json\"):\n",
        "    \"\"\"\n",
        "    Save user feedback to a JSON file for persistence across sessions.\n",
        "    Each feedback entry is stored as a separate line (JSONL format).\n",
        "    \n",
        "    Args:\n",
        "        feedback (dict): The feedback dictionary to store\n",
        "        file_path (str): Path where feedback file should be saved\n",
        "    \"\"\"\n",
        "    # Create the directory structure if it doesn't exist\n",
        "    # This ensures we can write to the file even if the folder is missing\n",
        "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "    # Create an empty file if it doesn't exist yet\n",
        "    # This prevents FileNotFoundError on first run\n",
        "    if not os.path.exists(file_path):\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            pass  # Just create the empty file\n",
        "\n",
        "    # Append the new feedback to the file\n",
        "    # We use append mode ('a') to keep all previous feedback\n",
        "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
        "        # Write feedback as JSON on a single line\n",
        "        json.dump(feedback, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")  # Add newline to separate entries\n",
        "    \n",
        "    print(f\"Feedback stored successfully to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_feedback_data(file_path=\"data/feedback_data.json\"):\n",
        "    \"\"\"\n",
        "    Load all stored feedback from the JSON file.\n",
        "    \n",
        "    Args:\n",
        "        file_path (str): Path to the feedback file\n",
        "    \n",
        "    Returns:\n",
        "        list: List of all feedback dictionaries\n",
        "    \"\"\"\n",
        "    feedback_data = []  # Initialize empty list to store feedback\n",
        "    \n",
        "    try:\n",
        "        # Try to open and read the feedback file\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            # Read each line as a separate JSON object\n",
        "            for line_number, line in enumerate(f, 1):\n",
        "                try:\n",
        "                    # Skip empty lines\n",
        "                    if line.strip():\n",
        "                        # Parse the JSON data and add to our list\n",
        "                        feedback_data.append(json.loads(line.strip()))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Warning: Could not parse line {line_number}: {e}\")\n",
        "                    continue  # Skip malformed lines\n",
        "                    \n",
        "    except FileNotFoundError:\n",
        "        # If file doesn't exist yet, that's okay - we're starting fresh\n",
        "        print(\"No feedback data file found. Starting with empty feedback.\")\n",
        "    \n",
        "    print(f\"Loaded {len(feedback_data)} feedback entries\")\n",
        "    return feedback_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Relevance Score Adjustment Function\n",
        "\n",
        "This is one of the core functions that makes our RAG system adaptive. It uses past feedback to adjust how relevant each document appears for new queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a Pydantic model to ensure LLM returns properly structured responses\n",
        "class Response(BaseModel):\n",
        "    answer: str = Field(..., title=\"The answer to the question. The options can be only 'Yes' or 'No'\")\n",
        "\n",
        "def adjust_relevance_scores(query: str, docs: List[Any], feedback_data: List[Dict[str, Any]]) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Adjust document relevance scores based on historical user feedback.\n",
        "    \n",
        "    This function:\n",
        "    1. Takes the current user query and retrieved documents\n",
        "    2. Looks at all past feedback to find relevant experiences\n",
        "    3. Uses an LLM to determine if past feedback applies to current documents\n",
        "    4. Adjusts document scores based on feedback quality\n",
        "    5. Returns re-ranked documents\n",
        "    \n",
        "    Args:\n",
        "        query (str): The current user question\n",
        "        docs (List[Any]): List of (document, similarity_score) tuples from retrieval\n",
        "        feedback_data (List[Dict]): All historical feedback data\n",
        "    \n",
        "    Returns:\n",
        "        List[Document]: Re-ranked documents with adjusted relevance scores\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create a prompt template for the LLM to evaluate feedback relevance\n",
        "    # This prompt asks the LLM to determine if past feedback is relevant to current context\n",
        "    relevance_prompt = PromptTemplate(\n",
        "        input_variables=[\"query\", \"feedback_query\", \"doc_content\", \"feedback_response\"],  # type: ignore\n",
        "        template=\"\"\"\n",
        "        Determine if the following feedback response is relevant to the current query and document content.\n",
        "        You are also provided with the feedback's original query for context.\n",
        "        \n",
        "        Current query: {query}\n",
        "        Original feedback query: {feedback_query}\n",
        "        Document content (first 1000 chars): {doc_content}\n",
        "        Feedback response: {feedback_response}\n",
        "        \n",
        "        Consider the feedback relevant if:\n",
        "        - The topics are similar between current and feedback queries\n",
        "        - The document content relates to the feedback response\n",
        "        - The feedback could inform the quality of this document for the current query\n",
        "        \n",
        "        Is this feedback relevant? Respond with only 'Yes' or 'No'.\n",
        "        \"\"\"\n",
        "    )\n",
        "    \n",
        "    # Initialize a more capable LLM for relevance evaluation\n",
        "    # Using GPT-4o for better reasoning about relevance\n",
        "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)  # type: ignore\n",
        "\n",
        "    # Create a chain that combines our prompt with structured output\n",
        "    # This ensures we get back a properly formatted Yes/No answer\n",
        "    relevance_chain = relevance_prompt | llm.with_structured_output(Response)\n",
        "\n",
        "    final_docs = []  # List to store documents with adjusted scores\n",
        "    \n",
        "    # Process each retrieved document\n",
        "    for doc, score in docs:\n",
        "        # Store the original similarity score in document metadata\n",
        "        doc.metadata[\"original_score\"] = score\n",
        "        doc.metadata[\"relevance_score\"] = score  # Start with original score\n",
        "        \n",
        "        relevant_feedback = []  # Collect feedback relevant to this document\n",
        "        \n",
        "        # Check each piece of historical feedback for relevance\n",
        "        for feedback in feedback_data:\n",
        "            # Prepare input for the LLM relevance checker\n",
        "            input_data = {\n",
        "                \"query\": query,\n",
        "                \"feedback_query\": feedback['query'],\n",
        "                \"doc_content\": doc.page_content[:1000],  # Limit to first 1000 chars to stay within token limits\n",
        "                \"feedback_response\": feedback['response']\n",
        "            }\n",
        "            \n",
        "            # Ask the LLM if this feedback is relevant\n",
        "            try:\n",
        "                result = relevance_chain.invoke(input_data).answer  # type: ignore\n",
        "                \n",
        "                # If LLM says the feedback is relevant, include it\n",
        "                if result.lower() == 'yes':\n",
        "                    relevant_feedback.append(feedback)\n",
        "                    print(f\"Found relevant feedback for document: {feedback['query'][:50]}...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error checking relevance: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Adjust the relevance score based on relevant feedback\n",
        "        if relevant_feedback:\n",
        "            # Calculate average relevance score from feedback (1-5 scale)\n",
        "            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n",
        "            \n",
        "            # Calculate average quality score from feedback (1-5 scale)\n",
        "            avg_quality = sum(f['quality'] for f in relevant_feedback) / len(relevant_feedback)\n",
        "            \n",
        "            # Combine relevance and quality for final adjustment\n",
        "            feedback_score = (avg_relevance + avg_quality) / 2\n",
        "            \n",
        "            # Adjust the original similarity score\n",
        "            # Score > 3 (neutral) boosts the document, score < 3 reduces it\n",
        "            adjustment_factor = (feedback_score / 3.0)  # 3 is neutral on 1-5 scale\n",
        "            doc.metadata['relevance_score'] = score * adjustment_factor\n",
        "            \n",
        "            # Store feedback info in metadata for debugging\n",
        "            doc.metadata['feedback_count'] = len(relevant_feedback)\n",
        "            doc.metadata['avg_feedback_score'] = feedback_score\n",
        "            \n",
        "            print(f\"Adjusted document score from {score:.3f} to {doc.metadata['relevance_score']:.3f} \"\n",
        "                  f\"based on {len(relevant_feedback)} feedback entries (avg score: {feedback_score:.2f})\")\n",
        "        \n",
        "        final_docs.append(doc)\n",
        "    \n",
        "    # Re-rank documents based on adjusted relevance scores\n",
        "    # Sort in descending order (highest relevance first)\n",
        "    sorted_docs = sorted(final_docs, key=lambda x: x.metadata['relevance_score'], reverse=True)\n",
        "    \n",
        "    print(f\"\\nDocument ranking after feedback adjustment:\")\n",
        "    for i, doc in enumerate(sorted_docs[:3]):  # Show top 3\n",
        "        original = doc.metadata.get('original_score', 0)\n",
        "        adjusted = doc.metadata.get('relevance_score', 0)\n",
        "        print(f\"{i+1}. Score: {original:.3f} -> {adjusted:.3f} | {doc.page_content[:100]}...\")\n",
        "    \n",
        "    return sorted_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: How Relevance Score Adjustment Works\n",
        "\n",
        "Let's see the relevance score adjustment in action with a concrete example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXAMPLE: Demonstrating Relevance Score Adjustment\n",
        "print(\"=== EXAMPLE: Relevance Score Adjustment ===\")\n",
        "\n",
        "# Step 1: Create some sample feedback data to simulate past interactions\n",
        "sample_feedback = [\n",
        "    {\n",
        "        \"query\": \"What is supervised learning?\",\n",
        "        \"response\": \"Supervised learning is a type of machine learning where algorithms learn from labeled training data...\",\n",
        "        \"relevance\": 5,\n",
        "        \"quality\": 4,\n",
        "        \"comments\": \"Very helpful explanation\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Explain neural networks\",\n",
        "        \"response\": \"Neural networks are computing systems inspired by biological neural networks...\",\n",
        "        \"relevance\": 3,\n",
        "        \"quality\": 2,\n",
        "        \"comments\": \"Too technical, hard to understand\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What are the types of machine learning?\",\n",
        "        \"response\": \"The main types are supervised, unsupervised, and reinforcement learning...\",\n",
        "        \"relevance\": 5,\n",
        "        \"quality\": 5,\n",
        "        \"comments\": \"Perfect answer, covered all types clearly\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Step 2: Test with a new query similar to our feedback\n",
        "test_query = \"What is machine learning and its types?\"\n",
        "\n",
        "print(f\"\\nTesting query: '{test_query}'\")\n",
        "print(\"\\nStep 1: Getting initial document retrieval...\")\n",
        "\n",
        "# Get initial retrieval results (documents with similarity scores)\n",
        "docs_with_scores = retriever.vectorstore.similarity_search_with_score(test_query, k=5)\n",
        "\n",
        "print(f\"Retrieved {len(docs_with_scores)} documents with original scores:\")\n",
        "for i, (doc, score) in enumerate(docs_with_scores):\n",
        "    print(f\"{i+1}. Original Score: {score:.3f} | Content: {doc.page_content[:100]}...\")\n",
        "\n",
        "print(\"\\nStep 2: Applying feedback-based relevance adjustment...\")\n",
        "\n",
        "# Apply relevance score adjustment using our sample feedback\n",
        "adjusted_docs = adjust_relevance_scores(test_query, docs_with_scores, sample_feedback)\n",
        "\n",
        "print(\"\\nStep 3: Comparison of before and after ranking:\")\n",
        "print(\"\\nBEFORE (original similarity scores):\")\n",
        "for i, (doc, score) in enumerate(docs_with_scores[:3]):\n",
        "    print(f\"{i+1}. Score: {score:.3f}\")\n",
        "\n",
        "print(\"\\nAFTER (feedback-adjusted scores):\")\n",
        "for i, doc in enumerate(adjusted_docs[:3]):\n",
        "    original = doc.metadata.get('original_score', 0)\n",
        "    adjusted = doc.metadata.get('relevance_score', 0)\n",
        "    feedback_count = doc.metadata.get('feedback_count', 0)\n",
        "    print(f\"{i+1}. Score: {original:.3f} -> {adjusted:.3f} (based on {feedback_count} feedback entries)\")\n",
        "\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"- Documents get boosted if past feedback was positive (relevance/quality > 3)\")\n",
        "print(\"- Documents get penalized if past feedback was negative (relevance/quality < 3)\")\n",
        "print(\"- Only feedback deemed relevant by the LLM affects the scores\")\n",
        "print(\"- This helps prioritize documents that previously led to good responses\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Index Fine-tuning Function\n",
        "\n",
        "This function periodically rebuilds the vector index to include high-quality question-answer pairs from user feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fine_tune_index(feedback_data: List[Dict[str, Any]], base_docs: List[Any], quality_threshold: int = 4) -> Any:\n",
        "    \"\"\"\n",
        "    Rebuild the vectorstore to include high-quality feedback as additional knowledge.\n",
        "    \n",
        "    This function:\n",
        "    1. Filters feedback to find high-quality question-answer pairs\n",
        "    2. Converts good feedback into new \"documents\"\n",
        "    3. Combines original documents with feedback-derived documents\n",
        "    4. Rebuilds the entire vectorstore with enhanced knowledge\n",
        "    \n",
        "    Args:\n",
        "        feedback_data (List[Dict]): All collected feedback\n",
        "        base_docs (List[Document]): Original document chunks\n",
        "        quality_threshold (int): Minimum score for feedback to be included (default: 4)\n",
        "    \n",
        "    Returns:\n",
        "        Chroma: New vectorstore with enhanced knowledge base\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"Starting index fine-tuning with {len(feedback_data)} feedback entries...\")\n",
        "    \n",
        "    # Step 1: Filter for high-quality feedback\n",
        "    # We only want to include feedback where both relevance and quality are high\n",
        "    good_responses = [\n",
        "        f for f in feedback_data \n",
        "        if f['relevance'] >= quality_threshold and f['quality'] >= quality_threshold\n",
        "    ]\n",
        "    \n",
        "    print(f\"Found {len(good_responses)} high-quality feedback entries (threshold: {quality_threshold})\")\n",
        "    \n",
        "    # Step 2: Convert high-quality feedback into new \"documents\"\n",
        "    additional_docs = []\n",
        "    for i, feedback in enumerate(good_responses):\n",
        "        # Combine the question and answer into a single document\n",
        "        # This creates a Q&A knowledge base from user interactions\n",
        "        combined_text = f\"Question: {feedback['query']}\\n\\nAnswer: {feedback['response']}\"\n",
        "        \n",
        "        # Create a new Document object with metadata\n",
        "        new_doc = Document(\n",
        "            page_content=combined_text,\n",
        "            metadata={\n",
        "                \"source\": \"user_feedback\",  # Mark this as feedback-derived\n",
        "                \"feedback_id\": i,\n",
        "                \"relevance_score\": feedback['relevance'],\n",
        "                \"quality_score\": feedback['quality'],\n",
        "                \"type\": \"qa_pair\"  # Distinguish from original documents\n",
        "            }\n",
        "        )\n",
        "        additional_docs.append(new_doc)\n",
        "        \n",
        "        print(f\"Added Q&A pair {i+1}: {feedback['query'][:60]}...\")\n",
        "\n",
        "    # Step 3: Combine original documents with feedback-derived documents\n",
        "    all_docs = base_docs + additional_docs\n",
        "    \n",
        "    print(f\"\\nCombining knowledge base:\")\n",
        "    print(f\"- Original documents: {len(base_docs)}\")\n",
        "    print(f\"- Feedback-derived documents: {len(additional_docs)}\")\n",
        "    print(f\"- Total documents: {len(all_docs)}\")\n",
        "\n",
        "    # Step 4: Rebuild the vectorstore with enhanced knowledge\n",
        "    # Create new embeddings for all documents (original + feedback)\n",
        "    embedding = OpenAIEmbeddings()\n",
        "    persist_directory = 'docs/chroma_enhanced/'  # Use different directory for enhanced version\n",
        "    \n",
        "    print(\"\\nRebuilding vectorstore with enhanced knowledge base...\")\n",
        "    \n",
        "    # Create the enhanced vectorstore\n",
        "    new_vectorstore = Chroma.from_documents(\n",
        "        documents=all_docs,\n",
        "        embedding=embedding,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "    \n",
        "    # Persist the vectorstore to disk for future use\n",
        "    new_vectorstore.persist()\n",
        "    \n",
        "    print(f\"Enhanced vectorstore created with {new_vectorstore._collection.count()} total embeddings\")\n",
        "    print(f\"Vectorstore saved to: {persist_directory}\")\n",
        "    \n",
        "    return new_vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: How Index Fine-tuning Works\n",
        "\n",
        "Let's demonstrate the fine-tuning process with concrete examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXAMPLE: Demonstrating Index Fine-tuning\n",
        "print(\"=== EXAMPLE: Index Fine-tuning Process ===\")\n",
        "\n",
        "# Step 1: Create comprehensive sample feedback data\n",
        "sample_feedback_for_tuning = [\n",
        "    {\n",
        "        \"query\": \"What is the difference between supervised and unsupervised learning?\",\n",
        "        \"response\": \"Supervised learning uses labeled data to train models, while unsupervised learning finds patterns in unlabeled data. Supervised learning includes classification and regression, while unsupervised includes clustering and dimensionality reduction.\",\n",
        "        \"relevance\": 5,\n",
        "        \"quality\": 5,\n",
        "        \"comments\": \"Excellent explanation with clear distinctions\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How does gradient descent work?\",\n",
        "        \"response\": \"Gradient descent is an optimization algorithm that iteratively adjusts model parameters to minimize the cost function by moving in the direction of steepest descent.\",\n",
        "        \"relevance\": 4,\n",
        "        \"quality\": 4,\n",
        "        \"comments\": \"Good technical explanation\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is overfitting?\",\n",
        "        \"response\": \"Overfitting occurs when a model performs well on training data but poorly on new, unseen data. It happens when the model is too complex and learns noise rather than patterns.\",\n",
        "        \"relevance\": 5,\n",
        "        \"quality\": 4,\n",
        "        \"comments\": \"Clear and practical explanation\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Explain deep learning\",\n",
        "        \"response\": \"Very technical explanation that was hard to follow...\",\n",
        "        \"relevance\": 2,\n",
        "        \"quality\": 2,\n",
        "        \"comments\": \"Too complicated, needs simpler explanation\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"\\nInput: {len(sample_feedback_for_tuning)} feedback entries\")\n",
        "for i, fb in enumerate(sample_feedback_for_tuning):\n",
        "    print(f\"{i+1}. Query: {fb['query'][:50]}... | Relevance: {fb['relevance']} | Quality: {fb['quality']}\")\n",
        "\n",
        "print(\"\\nStep 1: Filtering high-quality feedback (threshold: 4)...\")\n",
        "\n",
        "# Show the filtering process\n",
        "quality_threshold = 4\n",
        "high_quality_feedback = [\n",
        "    f for f in sample_feedback_for_tuning \n",
        "    if f['relevance'] >= quality_threshold and f['quality'] >= quality_threshold\n",
        "]\n",
        "\n",
        "print(f\"Filtered to {len(high_quality_feedback)} high-quality entries:\")\n",
        "for i, fb in enumerate(high_quality_feedback):\n",
        "    print(f\"{i+1}. {fb['query'][:60]}... (R:{fb['relevance']}, Q:{fb['quality']})\")\n",
        "\n",
        "print(\"\\nStep 2: Converting feedback to documents...\")\n",
        "\n",
        "# Show how feedback becomes documents\n",
        "for i, fb in enumerate(high_quality_feedback[:2]):  # Show first 2 examples\n",
        "    combined_text = f\"Question: {fb['query']}\\n\\nAnswer: {fb['response']}\"\n",
        "    print(f\"\\nNew Document {i+1}:\")\n",
        "    print(f\"Content: {combined_text[:200]}...\")\n",
        "    print(f\"Metadata: source=user_feedback, relevance={fb['relevance']}, quality={fb['quality']}\")\n",
        "\n",
        "print(\"\\nStep 3: Simulating vectorstore enhancement...\")\n",
        "\n",
        "# Get current vectorstore stats\n",
        "original_count = vectorstore._collection.count() if hasattr(vectorstore, '_collection') else len(splits)\n",
        "enhanced_count = original_count + len(high_quality_feedback)\n",
        "\n",
        "print(f\"Before fine-tuning: {original_count} documents\")\n",
        "print(f\"Adding: {len(high_quality_feedback)} Q&A pairs from feedback\")\n",
        "print(f\"After fine-tuning: {enhanced_count} documents (projected)\")\n",
        "\n",
        "print(\"\\nStep 4: Benefits of fine-tuning:\")\n",
        "print(\"- User-validated Q&A pairs become searchable knowledge\")\n",
        "print(\"- Future similar queries will find these proven good answers\")\n",
        "print(\"- System learns from successful interactions\")\n",
        "print(\"- Knowledge base becomes more comprehensive over time\")\n",
        "\n",
        "enhanced_vectorstore = fine_tune_index(sample_feedback_for_tuning, splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
