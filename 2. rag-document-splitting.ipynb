{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 📄 Text Splitting Tutorial for RAG Systems\n",
    "\n",
    "## Why Do We Need to Split Text?\n",
    "\n",
    "Imagine you have a huge book, and you want to find specific information quickly. Instead of reading the entire book every time, you'd prefer to look at individual chapters or pages, right? \n",
    "\n",
    "That's exactly what text splitting does for AI systems! 🤖\n",
    "\n",
    "### The Problem:\n",
    "- 📚 Documents are often too large for AI models to process at once\n",
    "- 🧠 AI models have limited \"memory\" (context windows)\n",
    "- 🔍 We want to find relevant information quickly\n",
    "- ⚡ Processing large texts is slow and expensive\n",
    "\n",
    "### The Solution:\n",
    "- ✂️ Break large documents into smaller, manageable chunks\n",
    "- 🎯 Each chunk contains related information\n",
    "- 🔄 Chunks can overlap to preserve context\n",
    "- 🔍 Search becomes faster and more accurate\n",
    "\n",
    "Let's learn how to do this step by step! 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bced7e",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Before we start, let's install the required packages and set up our environment.\n",
    "\n",
    "**Why these packages?**\n",
    "- `langchain_community`: Community-contributed components\n",
    "- `langchain`: Core LangChain functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain_community\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_splitting",
   "metadata": {},
   "source": [
    "## 🔧 Basic Text Splitting Setup\n",
    "\n",
    "First, let's import the tools we need and understand the basic concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the text splitting tools from LangChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# Set up our splitting parameters\n",
    "chunk_size = 26      # Maximum characters per chunk (like a page limit)\n",
    "chunk_overlap = 4    # How many characters to repeat between chunks (like bookmarks)\n",
    "\n",
    "# Create two different types of splitters\n",
    "# Think of these as different ways to cut a cake:\n",
    "\n",
    "# 1. Recursive Splitter (Smart splitter - tries different ways to split)\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# 2. Character Splitter (Simple splitter - splits at specific characters)\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "print(\"✅ Text splitters are ready!\")\n",
    "print(f\"📏 Chunk size: {chunk_size} characters\")\n",
    "print(f\"🔄 Overlap: {chunk_overlap} characters\")\n",
    "print()\n",
    "print(\"💡 What do these parameters mean?\")\n",
    "print(\"   • Chunk size: Maximum characters per chunk (like a page limit)\")\n",
    "print(\"   • Overlap: Characters shared between chunks (like bookmarks)\")\n",
    "print(\"   • We're using small numbers for demonstration - real apps use 500-2000!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple_examples",
   "metadata": {},
   "source": [
    "## 🧪 Simple Examples to Understand Splitting\n",
    "\n",
    "Let's start with very simple examples to see how splitting works. We'll use the alphabet to make it easy to count characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: A string that's exactly 26 characters (same as our chunk_size)\n",
    "text1 = 'abcdefghijklmnopqrstuvwxyz'\n",
    "print(f\"📝 Text 1: '{text1}'\")\n",
    "print(f\"📏 Length: {len(text1)} characters\")\n",
    "\n",
    "# Try to split it\n",
    "result1 = r_splitter.split_text(text1)\n",
    "print(f\"🔄 Split result: {result1}\")\n",
    "print(f\"📊 Number of chunks: {len(result1)}\")\n",
    "print(f\"💭 Why only 1 chunk? Because the text fits exactly in our chunk size!\")\n",
    "print(f\"💭 No splitting needed when text ≤ chunk_size\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: A string that's longer than our chunk_size\n",
    "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\n",
    "print(f\"📝 Text 2: '{text2}'\")\n",
    "print(f\"📏 Length: {len(text2)} characters\")\n",
    "\n",
    "# Split it\n",
    "result2 = r_splitter.split_text(text2)\n",
    "print(f\"🔄 Split result: {result2}\")\n",
    "print(f\"📊 Number of chunks: {len(result2)}\")\n",
    "print()\n",
    "\n",
    "# Let's examine each chunk\n",
    "print(\"🔍 Let's examine each chunk:\")\n",
    "for i, chunk in enumerate(result2):\n",
    "    print(f\"   Chunk {i+1}: '{chunk}' ({len(chunk)} chars)\")\n",
    "\n",
    "print()\n",
    "print(\"💭 Notice the overlap: 'wxyz' appears in both chunks!\")\n",
    "print(\"💭 This is our 4-character overlap preserving context between chunks\")\n",
    "print(\"💭 Chunk 2 starts with the last 4 characters of chunk 1\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overlap_explanation",
   "metadata": {},
   "source": [
    "## 🔄 Understanding Overlap in Detail\n",
    "\n",
    "**Overlap** is like having a bookmark that shows you a bit of the previous page. It helps maintain context between chunks. Let's see this with spaces to make it clearer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overlap_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Text with spaces (easier to see how splitting works)\n",
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "print(f\"📝 Text 3: '{text3}'\")\n",
    "print(f\"📏 Length: {len(text3)} characters\")\n",
    "print()\n",
    "\n",
    "# Split with recursive splitter\n",
    "print(\"🔄 Using Recursive Splitter:\")\n",
    "result3_r = r_splitter.split_text(text3)\n",
    "for i, chunk in enumerate(result3_r):\n",
    "    print(f\"   Chunk {i+1}: '{chunk}' (length: {len(chunk)})\")\n",
    "print()\n",
    "\n",
    "# Split with character splitter\n",
    "print(\"🔄 Using Character Splitter:\")\n",
    "result3_c = c_splitter.split_text(text3)\n",
    "for i, chunk in enumerate(result3_c):\n",
    "    print(f\"   Chunk {i+1}: '{chunk}' (length: {len(chunk)})\")\n",
    "print()\n",
    "\n",
    "print(\"💭 Big Difference! Why?\")\n",
    "print(\"💭 Recursive Splitter: Looks for natural break points (spaces, punctuation)\")\n",
    "print(\"💭 Character Splitter: Only splits when it absolutely has to\")\n",
    "print(\"💭 In this case, Character Splitter kept everything together because it could fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_separator",
   "metadata": {},
   "source": [
    "## ✂️ Custom Splitting Rules\n",
    "\n",
    "We can tell the splitter exactly where to cut by specifying a **separator**. Think of separators as \"cut here\" instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a character splitter that splits at spaces\n",
    "c_splitter_space = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator=' '  # Split at spaces\n",
    ")\n",
    "\n",
    "print(\"✂️ Using Character Splitter with space separator:\")\n",
    "result3_space = c_splitter_space.split_text(text3)\n",
    "for i, chunk in enumerate(result3_space):\n",
    "    print(f\"   Chunk {i+1}: '{chunk}' (length: {len(chunk)})\")\n",
    "\n",
    "print()\n",
    "print(\"🎯 Comparing all three approaches:\")\n",
    "print(f\"   Default Character Splitter: {len(result3_c)} chunk (keeps everything together)\")\n",
    "print(f\"   Space-based Character Splitter: {len(result3_space)} chunks (splits at spaces)\")\n",
    "print(f\"   Recursive Splitter: {len(result3_r)} chunks (smart splitting)\")\n",
    "print()\n",
    "print(\"💭 The separator tells the splitter WHERE it's allowed to cut!\")\n",
    "print(\"💭 Without a separator, it can only cut anywhere (not ideal)\")\n",
    "print(\"💭 With separator=' ', it can only cut at spaces (much better!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real_world",
   "metadata": {},
   "source": [
    "## 🌟 Real-World Example: Splitting a Paragraph\n",
    "\n",
    "Now let's work with actual text that you might encounter in real documents. This will show how splitting works with realistic content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "real_text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A real paragraph about document structure\n",
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\"\n",
    "\n",
    "print(\"📄 Here's our sample text:\")\n",
    "print(some_text)\n",
    "print()\n",
    "print(f\"📏 Text length: {len(some_text)} characters\")\n",
    "print(f\"📊 With our tiny chunk size (26), this would create ~{len(some_text)//26} chunks!\")\n",
    "print(f\"📊 That's why we'll use larger chunk sizes for real text...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "real_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splitters with larger chunk sizes for this longer text\n",
    "c_splitter_real = CharacterTextSplitter(\n",
    "    chunk_size=450,     # Bigger chunks for longer text\n",
    "    chunk_overlap=0,    # No overlap for now\n",
    "    separator=' '       # Split at spaces\n",
    ")\n",
    "\n",
    "r_splitter_real = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0,\n",
    "    # List of separators to try (in order of preference)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try paragraphs, then lines, then words, then characters\n",
    ")\n",
    "\n",
    "print(\"✂️ Character Splitter Results:\")\n",
    "c_result = c_splitter_real.split_text(some_text)\n",
    "for i, chunk in enumerate(c_result):\n",
    "    print(f\"Chunk {i+1}: '{chunk}' (length: {len(chunk)})\")\n",
    "\n",
    "print(\"\\n🔄 Recursive Splitter Results:\")\n",
    "r_result = r_splitter_real.split_text(some_text)\n",
    "for i, chunk in enumerate(r_result):\n",
    "    print(f\"Chunk {i+1}: '{chunk}' (length: {len(chunk)})\")\n",
    "\n",
    "print(\"\\n💭 Key Differences:\")\n",
    "print(\"💭 Character Splitter: Cut awkwardly in the middle of 'have a'\")\n",
    "print(\"💭 Recursive Splitter: Made cleaner cuts at sentence/paragraph boundaries\")\n",
    "print(\"💭 Recursive is generally better for readability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart_splitting",
   "metadata": {},
   "source": [
    "## 🎯 Smart Splitting: Keeping Sentences Together\n",
    "\n",
    "Let's make our splitter even smarter by teaching it to split at sentence boundaries first. This preserves meaning much better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentence_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter that prefers to split at sentence endings\n",
    "r_splitter_smart = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,      # Smaller chunks to see the effect\n",
    "    chunk_overlap=0,\n",
    "    # Try to split at these points (in order):\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Paragraphs, lines, sentences, words, characters\n",
    ")\n",
    "\n",
    "print(\"🎯 Smart Sentence-Aware Splitting:\")\n",
    "smart_result = r_splitter_smart.split_text(some_text)\n",
    "for i, chunk in enumerate(smart_result):\n",
    "    print(f\"\\nChunk {i+1} (length: {len(chunk)}):\")\n",
    "    print(f\"'{chunk}'\")\n",
    "\n",
    "print(\"\\n✨ Benefits of Sentence-Aware Splitting:\")\n",
    "print(\"   • Each chunk contains complete thoughts\")\n",
    "print(\"   • No sentences are cut in half\")\n",
    "print(\"   • Better for AI understanding and retrieval\")\n",
    "print(\"   • More natural reading experience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overlap_demonstration",
   "metadata": {},
   "source": [
    "## 🔄 Overlap in Action: Why It Matters\n",
    "\n",
    "Let's see why overlap is crucial for maintaining context across chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overlap_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splitters with and without overlap to compare\n",
    "no_overlap_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,  # No overlap\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "with_overlap_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=30,  # 30 character overlap\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"🚫 WITHOUT Overlap:\")\n",
    "no_overlap_result = no_overlap_splitter.split_text(some_text)\n",
    "for i, chunk in enumerate(no_overlap_result[:3]):  # Show first 3 chunks\n",
    "    print(f\"\\nChunk {i+1}: '{chunk}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*36)\n",
    "\n",
    "print(\"\\n✅ WITH Overlap (30 characters):\")\n",
    "with_overlap_result = with_overlap_splitter.split_text(some_text)\n",
    "for i, chunk in enumerate(with_overlap_result[:3]):  # Show first 3 chunks\n",
    "    print(f\"\\nChunk {i+1}: '{chunk}'\")\n",
    "\n",
    "print(\"\\n💡 Notice how overlap helps:\")\n",
    "print(\"   • 'which idea's are related' bridges chunks 1 & 2\")\n",
    "print(\"   • 'Paragraphs form a' bridges chunks 2 & 3\")\n",
    "print(\"   • Context is preserved across chunk boundaries\")\n",
    "print(\"   • AI can better understand relationships between chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pdf_splitting",
   "metadata": {},
   "source": [
    "## 📄 Real Document Splitting: Working with Files\n",
    "\n",
    "Now let's see how to apply text splitting to real documents. We'll demonstrate with a simulated PDF loading process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pdf_simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate loading a PDF document with multiple pages\n",
    "# In real applications, you'd use: from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Simulate PDF pages (in real code, this would come from PyPDFLoader)\n",
    "simulated_pdf_pages = [\n",
    "    {\n",
    "        'page_content': '''Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. The field has gained tremendous popularity in recent years due to advances in computing power, data availability, and algorithmic improvements.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Uses labeled data to train models\n",
    "2. Unsupervised Learning: Finds patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learns through interaction with environment\n",
    "\n",
    "Applications include image recognition, natural language processing, recommendation systems, autonomous vehicles, medical diagnosis, and financial fraud detection. The impact of machine learning continues to grow across industries.\n",
    "\n",
    "Key algorithms include linear regression, decision trees, neural networks, support vector machines, and ensemble methods. Each algorithm has strengths and weaknesses depending on the problem type and data characteristics.''',\n",
    "        'metadata': {'page': 1}\n",
    "    },\n",
    "    {\n",
    "        'page_content': '''Deep Learning and Neural Networks\n",
    "\n",
    "Deep learning represents a significant breakthrough in machine learning, using neural networks with multiple layers to model complex patterns in data. These networks can automatically learn hierarchical representations without manual feature engineering.\n",
    "\n",
    "Architecture Types:\n",
    "- Convolutional Neural Networks (CNNs) for image processing\n",
    "- Recurrent Neural Networks (RNNs) for sequential data\n",
    "- Transformers for natural language understanding\n",
    "- Generative Adversarial Networks (GANs) for content creation\n",
    "\n",
    "Training deep networks requires substantial computational resources and large datasets. Graphics Processing Units (GPUs) have become essential for training complex models efficiently.\n",
    "\n",
    "Recent advances include attention mechanisms, transfer learning, and pre-trained models that can be fine-tuned for specific tasks. These developments have democratized access to powerful AI capabilities.''',\n",
    "        'metadata': {'page': 2}\n",
    "    },\n",
    "    {\n",
    "        'page_content': '''Future of AI and Ethical Considerations\n",
    "\n",
    "The future of artificial intelligence holds both tremendous promise and significant challenges. As AI systems become more capable and widespread, we must address important ethical considerations.\n",
    "\n",
    "Key Ethical Issues:\n",
    "- Bias and fairness in AI decisions\n",
    "- Privacy and data protection\n",
    "- Job displacement and economic impact\n",
    "- Transparency and explainability\n",
    "- AI safety and alignment with human values\n",
    "\n",
    "Emerging trends include federated learning, quantum machine learning, edge AI, and neuromorphic computing. These technologies promise to make AI more efficient, private, and accessible.\n",
    "\n",
    "Responsible AI development requires collaboration between technologists, policymakers, ethicists, and society. We must ensure that AI benefits all of humanity while minimizing potential risks and negative consequences.''',\n",
    "        'metadata': {'page': 3}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"📄 Simulated PDF Content ({len(simulated_pdf_pages)} pages):\")\n",
    "total_chars = 0\n",
    "for i, page in enumerate(simulated_pdf_pages):\n",
    "    page_length = len(page['page_content'])\n",
    "    total_chars += page_length\n",
    "    print(f\"\\nPage {i+1} ({page_length} chars):\")\n",
    "    print(page['page_content'])\n",
    "\n",
    "print(f\"\\n📊 Total content: {total_chars} characters across {len(simulated_pdf_pages)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pdf_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter for the PDF content\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Convert our simulated pages to Document objects (like LangChain does)\n",
    "pages = [Document(page_content=page['page_content'], metadata=page['metadata']) \n",
    "         for page in simulated_pdf_pages]\n",
    "\n",
    "# Create a practical text splitter for documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,       # Each chunk can be up to 500 characters\n",
    "    chunk_overlap=50,     # 50 characters overlap between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Smart splitting priority\n",
    "    length_function=len   # Use character count to measure length\n",
    ")\n",
    "\n",
    "# Split the PDF pages into smaller chunks\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"📊 Splitting Results:\")\n",
    "print(f\"   Original PDF: {len(pages)} pages\")\n",
    "print(f\"   After splitting: {len(docs)} chunks\")\n",
    "print(f\"   📈 We created {len(docs) - len(pages)} additional chunks for better processing!\")\n",
    "\n",
    "print(f\"\\n🔍 First few chunks:\")\n",
    "for i, doc in enumerate(docs[:3]):  # Show first 3 chunks\n",
    "    print(f\"\\nChunk {i+1} ({len(doc.page_content)} chars, from page {doc.metadata['page']}):\")\n",
    "    print(f\"'{doc.page_content}'\")\n",
    "\n",
    "print(\"\\n💡 Notice:\")\n",
    "print(\"   • Large pages were split into smaller, manageable chunks\")\n",
    "print(\"   • Each chunk maintains metadata about its source page\")\n",
    "print(\"   • Overlap preserves context between chunks\")\n",
    "print(\"   • Chunks are roughly equal in size for consistent processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token_splitting",
   "metadata": {},
   "source": [
    "## 🔢 Token-Based Splitting: Speaking AI's Language\n",
    "\n",
    "Sometimes we want to split based on **tokens** instead of characters. Tokens are the \"words\" that AI models actually understand:\n",
    "\n",
    "- 1 token ≈ 4 characters (rough estimate, varies by language)\n",
    "- AI models have specific token limits (e.g., 4,000 tokens for GPT-3.5)\n",
    "- This gives more precise control for AI applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "token_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the token-based splitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Create a splitter that splits by tokens (very small for demo)\n",
    "token_splitter_demo = TokenTextSplitter(\n",
    "    chunk_size=1,     # Just 1 token per chunk (for demonstration)\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "print(\"🔢 Token Splitting Demo:\")\n",
    "\n",
    "# Test with simple text\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The quick brown fox jumps over the lazy dog. Machine learning is fascinating.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\n📝 Test text: '{text}'\")\n",
    "    print(f\"📏 Character length: {len(text)}\")\n",
    "    \n",
    "    # Split into tokens\n",
    "    token_result = token_splitter_demo.split_text(text)\n",
    "    print(f\"🔢 Split into tokens: {token_result}\")\n",
    "    print(f\"📊 Number of tokens: {len(token_result)}\")\n",
    "\n",
    "print(\"\\n💭 Why only 1 token? Our chunk_size=1 is too small for realistic use!\")\n",
    "print(\"💭 Let's try with a more practical token count...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "# Now with a more practical token count\n",
    "practical_token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,    # 50 tokens per chunk (more realistic)\n",
    "    chunk_overlap=5   # 5 token overlap\n",
    ")\n",
    "\n",
    "print(\"🎯 Practical Token Splitting (50 tokens per chunk):\")\n",
    "\n",
    "# Apply to our PDF documents\n",
    "token_docs = practical_token_splitter.split_documents(pages)\n",
    "print(f\"\\nApplied to our PDF content:\")\n",
    "print(f\"📊 Original pages: {len(pages)}\")\n",
    "print(f\"📊 Token-based chunks: {len(token_docs)}\")\n",
    "\n",
    "print(f\"\\n📝 Sample token chunk:\")\n",
    "print(f\"'{token_docs[0].page_content}'\")\n",
    "print(f\"📏 Length: {len(token_docs[0].page_content)} characters\")\n",
    "print(f\"📋 Metadata: {token_docs[0].metadata}\")\n",
    "\n",
    "print(\"\\n💡 Token vs Character Splitting:\")\n",
    "print(f\"   • Character splitting: {len(docs)} chunks (500 chars each)\")\n",
    "print(f\"   • Token splitting: {len(token_docs)} chunks (50 tokens ≈ 200 chars each)\")\n",
    "print(\"   • Token splitting gives more uniform AI processing units\")\n",
    "print(\"   • Better for staying within model token limits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_splitting",
   "metadata": {},
   "source": [
    "## 📋 Structure-Aware Splitting: Preserving Document Organization\n",
    "\n",
    "When documents have structure (like headers in Markdown), we want to keep that structure information with our chunks. This is crucial for maintaining document context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "markdown_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the specialized markdown splitter\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "# Create a more realistic markdown document\n",
    "markdown_document = \"\"\"# AI Research Guide\n",
    "\n",
    "## Chapter 1: Introduction\n",
    "\n",
    "Artificial intelligence research is rapidly evolving. This guide covers key concepts and methodologies.\n",
    "\n",
    "### Section 1.1: Fundamentals\n",
    "\n",
    "Understanding the basics is crucial for any AI researcher.\n",
    "\n",
    "### Section 1.2: Tools\n",
    "\n",
    "Modern AI research requires specialized tools and frameworks.\n",
    "\n",
    "## Chapter 2: Methodologies\n",
    "\n",
    "Research methodologies in AI vary depending on the specific domain and objectives.\n",
    "\n",
    "### Section 2.1: Experimental Design\n",
    "\n",
    "Proper experimental design ensures reliable and reproducible results.\n",
    "\n",
    "## Chapter 3: Future Directions\n",
    "\n",
    "The future of AI research holds many exciting possibilities and challenges.\"\"\"\n",
    "\n",
    "print(\"📋 Sample Markdown Document:\")\n",
    "print(markdown_document)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "markdown_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which headers we want to split on\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),      # Main title\n",
    "    (\"##\", \"Header 2\"),     # Chapter\n",
    "    (\"###\", \"Header 3\"),    # Section\n",
    "]\n",
    "\n",
    "# Create the markdown splitter\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "print(f\"📊 Markdown splitting created {len(md_header_splits)} structured chunks:\\n\")\n",
    "\n",
    "for i, chunk in enumerate(md_header_splits):\n",
    "    print(f\"🔸 Chunk {i+1}:\")\n",
    "    print(f\"   Content: '{chunk.page_content.strip()}'\")\n",
    "    print(f\"   Headers: {chunk.metadata}\")\n",
    "    print()\n",
    "\n",
    "print(\"✨ Benefits of Header-Aware Splitting:\")\n",
    "print(\"   • Each chunk knows its place in the document hierarchy\")\n",
    "print(\"   • Content is grouped by logical structure, not just size\")\n",
    "print(\"   • AI can understand document organization\")\n",
    "print(\"   • Better retrieval based on document sections\")\n",
    "print(\"   • Maintains context of where information appears\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid_splitting",
   "metadata": {},
   "source": [
    "## 🔄 Hybrid Approach: Combining Header + Size Splitting\n",
    "\n",
    "Often, we want the best of both worlds: structure-aware splitting AND size control. Here's how to combine them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger markdown document to demonstrate hybrid splitting\n",
    "large_markdown = \"\"\"# Machine Learning Handbook\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Machine learning has revolutionized how we approach complex problems across various domains. From healthcare to finance, transportation to entertainment, ML algorithms are transforming industries and creating new possibilities for innovation and efficiency.\n",
    "\n",
    "The field encompasses various techniques and methodologies, each suited for different types of problems and data structures. Understanding these fundamentals is crucial for any practitioner.\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "Supervised learning uses labeled datasets to train algorithms that can make predictions or classifications on new, unseen data.\n",
    "\n",
    "### Classification\n",
    "\n",
    "Classification algorithms predict discrete categories or classes. Common algorithms include logistic regression, decision trees, random forests, and support vector machines. These methods are widely used in spam detection, image recognition, and medical diagnosis applications.\n",
    "\n",
    "### Regression\n",
    "\n",
    "Regression algorithms predict continuous numerical values. Linear regression, polynomial regression, and neural networks are popular choices for tasks like price prediction, weather forecasting, and stock market analysis.\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised learning finds hidden patterns in data without labeled examples. Clustering algorithms like K-means and hierarchical clustering group similar data points, while dimensionality reduction techniques like PCA help visualize high-dimensional data.\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "Successful machine learning projects require careful attention to data quality, feature engineering, model selection, and evaluation metrics. Cross-validation and proper testing procedures ensure robust and reliable results.\"\"\"\n",
    "\n",
    "print(f\"📄 Large Markdown Document Created ({len(large_markdown)} characters)\")\n",
    "\n",
    "# Step 1: Split by headers first\n",
    "header_chunks = markdown_splitter.split_text(large_markdown)\n",
    "print(f\"\\n🔄 Step 1: Header-based splitting created {len(header_chunks)} chunks\")\n",
    "\n",
    "# Step 2: Apply size-based splitting to chunks that are too large\n",
    "size_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,  # Maximum 300 characters per chunk\n",
    "    chunk_overlap=30,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"🔄 Step 2: Size-based splitting of large chunks...\")\n",
    "\n",
    "final_chunks = []\n",
    "for chunk in header_chunks:\n",
    "    if len(chunk.page_content) > 300:\n",
    "        # Split large chunks further\n",
    "        sub_chunks = size_splitter.split_documents([chunk])\n",
    "        final_chunks.extend(sub_chunks)\n",
    "    else:\n",
    "        # Keep small chunks as-is\n",
    "        final_chunks.append(chunk)\n",
    "\n",
    "print(f\"\\n📊 Final Results:\")\n",
    "print(f\"   After header splitting: {len(header_chunks)} chunks\")\n",
    "print(f\"   After size splitting: {len(final_chunks)} chunks\")\n",
    "print(f\"   📈 Added {len(final_chunks) - len(header_chunks)} chunks by splitting oversized sections\")\n",
    "\n",
    "print(f\"\\n🔍 Sample chunks:\")\n",
    "for i, chunk in enumerate(final_chunks[:2]):  # Show first 2\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk.page_content)} chars):\")\n",
    "    print(f\"Headers: {chunk.metadata}\")\n",
    "    print(f\"Content: '{chunk.page_content.strip()}'\")\n",
    "\n",
    "# Show one more interesting chunk\n",
    "if len(final_chunks) > 4:\n",
    "    chunk = final_chunks[4]\n",
    "    print(f\"\\nChunk 5 ({len(chunk.page_content)} chars):\")\n",
    "    print(f\"Headers: {chunk.metadata}\")\n",
    "    print(f\"Content: '{chunk.page_content.strip()}'\")\n",
    "\n",
    "print(\"\\n✅ Perfect! Each chunk:\")\n",
    "print(\"   • Respects document structure (headers preserved)\")\n",
    "print(\"   • Stays within size limits (≤300 characters)\")\n",
    "print(\"   • Maintains complete context information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways & Best Practices\n",
    "\n",
    "### ✅ What We've Learned:\n",
    "\n",
    "1. **Why Split Text?**\n",
    "   - AI models have memory limits (context windows)\n",
    "   - Smaller chunks = better, more precise retrieval\n",
    "   - Faster processing and lower costs\n",
    "   - Easier to find relevant information\n",
    "\n",
    "2. **Types of Splitters:**\n",
    "   - **Character Splitter**: Simple, splits at specific characters\n",
    "   - **Recursive Splitter**: Smart, tries multiple splitting strategies\n",
    "   - **Token Splitter**: Splits by AI \"words\" (tokens) - best for model limits\n",
    "   - **Markdown Splitter**: Preserves document structure and hierarchy\n",
    "\n",
    "3. **Important Parameters:**\n",
    "   - **Chunk Size**: How big each piece should be (500-2000 chars typical)\n",
    "   - **Overlap**: How much pieces should share (10-20% of chunk size)\n",
    "   - **Separators**: Where to make cuts (paragraphs > sentences > words > characters)\n",
    "\n",
    "### 🚀 Best Practices:\n",
    "\n",
    "1. **Choose the Right Splitter:**\n",
    "   - Use **RecursiveCharacterTextSplitter** for most cases\n",
    "   - Use **MarkdownHeaderTextSplitter** for structured documents\n",
    "   - Use **TokenTextSplitter** when working with AI model token limits\n",
    "   - Consider **hybrid approaches** for complex documents\n",
    "\n",
    "2. **Set Appropriate Sizes:**\n",
    "   - Chunk size: 500-1000 characters for general use, 200-2000 for specific needs\n",
    "   - Overlap: 10-20% of chunk size (50-200 characters typically)\n",
    "   - Token limits: Stay well below model limits (e.g., 1000 tokens for 4K model)\n",
    "\n",
    "3. **Optimize for Your Use Case:**\n",
    "   - **Q&A systems**: Smaller chunks (200-500 chars) for precise answers\n",
    "   - **Summarization**: Larger chunks (1000-2000 chars) for context\n",
    "   - **Search**: Medium chunks (500-1000 chars) for balance\n",
    "\n",
    "4. **Test and Iterate:**\n",
    "   - Try different settings with your actual documents\n",
    "   - Check if chunks make sense to humans\n",
    "   - Measure performance with your AI application\n",
    "   - A/B test different splitting strategies\n",
    "\n",
    "### 🔄 What Comes Next:\n",
    "1. **Embeddings**: Convert text chunks to vectors for similarity search\n",
    "2. **Vector Storage**: Store and index chunks efficiently (Pinecone, Weaviate, etc.)\n",
    "3. **Retrieval**: Find the most relevant chunks for user queries\n",
    "4. **Generation**: Use retrieved chunks to generate accurate answers\n",
    "\n",
    "### 📊 Quick Reference:\n",
    "\n",
    "```python\n",
    "# Most common setup for general use\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Good text splitting is like good organization - it makes everything else in your RAG pipeline work better! 🎉\n",
    "\n",
    "The key is finding the right balance for your specific documents and use case. Start with the defaults, then optimize based on your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
