{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Self-RAG?\n",
        "\n",
        "Think of Self-RAG like a really smart student taking a test. Before answering each question, the student decides: \"Do I already know this, or should I look it up in my textbook?\" If they need to look something up, they check if the information they found is actually helpful, then give the best possible answer.\n",
        "\n",
        "Self-RAG works the same way - it's an AI system that can decide whether it needs to search for information before answering your questions.\n",
        "\n",
        "## Why Do We Need This?\n",
        "\n",
        "Imagine you ask an AI two different questions:\n",
        "- \"What's 2 + 2?\" (Easy - no need to look anything up)\n",
        "- \"What's the latest news about climate change?\" (Complex - better look up recent information)\n",
        "\n",
        "**The Problem**: Many AI systems either:\n",
        "- Always look things up (even for simple math) - which is slow and unnecessary\n",
        "- Never look things up (even for recent events) - which can give outdated or wrong answers\n",
        "\n",
        "**Self-RAG's Solution**: Be smart about when to search and when not to!\n",
        "\n",
        "## How Does Self-RAG Work? (Step by Step)\n",
        "\n",
        "Let's follow what happens when you ask: *\"What are the health benefits of drinking green tea?\"*\n",
        "\n",
        "### Step 1: Should I Look This Up?\n",
        "The system thinks: *\"This question is about health facts that I should verify with reliable sources. YES, I should look this up.\"*\n",
        "\n",
        "### Step 2: Find Information\n",
        "The system searches through a database of articles, research papers, and reliable websites about green tea and health.\n",
        "\n",
        "### Step 3: Check If Information Is Useful\n",
        "The system looks at what it found and asks: *\"Is this information actually about green tea's health benefits, or is it about something else entirely?\"* It keeps only the relevant information.\n",
        "\n",
        "### Step 4: Create an Answer\n",
        "Using the good information it found, the system writes several possible answers to your question.\n",
        "\n",
        "### Step 5: Check the Answer Quality\n",
        "The system asks itself two important questions:\n",
        "- *\"Is my answer actually supported by the information I found?\"* (No making things up!)\n",
        "- *\"How helpful is this answer to the person who asked the question?\"* (Is it useful?)\n",
        "\n",
        "### Step 6: Pick the Best Answer\n",
        "Finally, it chooses the answer that is both well-supported by evidence and most helpful to you.\n",
        "\n",
        "## Real-World Example\n",
        "\n",
        "**Your Question**: \"How do I reset my iPhone?\"\n",
        "\n",
        "**Self-RAG's Process**:\n",
        "1. **Should I look this up?** *\"This is a technical question that might have changed with new iOS versions. YES, let me check current instructions.\"*\n",
        "2. **Search**: Finds current Apple support documentation\n",
        "3. **Check relevance**: *\"Yes, these are official iPhone reset instructions\"*\n",
        "4. **Generate answer**: Creates step-by-step instructions\n",
        "5. **Verify quality**: *\"This answer is supported by official Apple docs and directly answers the question\"*\n",
        "6. **Final answer**: Gives you accurate, up-to-date reset instructions\n",
        "\n",
        "## Why Is This Better?\n",
        "\n",
        "### **Smart Decision Making**\n",
        "- Doesn't waste time looking up simple things like \"What color is the sky?\"\n",
        "- Does look up complex or changing information like \"Current COVID-19 guidelines\"\n",
        "\n",
        "### **Quality Control**\n",
        "- Double-checks that answers are actually supported by reliable sources\n",
        "- Makes sure answers are helpful, not just technically correct\n",
        "\n",
        "### **Flexibility**\n",
        "- Can work with or without looking things up\n",
        "- Adapts to different types of questions\n",
        "\n",
        "### **More Accurate**\n",
        "- Reduces hallucinations (making things up)\n",
        "- Bases answers on verified information when needed\n",
        "\n",
        "## Think of It Like...\n",
        "\n",
        "**A Good Research Assistant**: Self-RAG is like having a research assistant who:\n",
        "- Knows when they need to check sources vs. when they already know the answer\n",
        "- Only brings you relevant information (not random stuff)\n",
        "- Double-checks their work before giving you the final answer\n",
        "- Always tries to give you the most helpful response possible\n",
        "\n",
        "## The Bottom Line\n",
        "\n",
        "Self-RAG makes AI systems smarter by teaching them to think before they answer. Just like how the best students know when to rely on their memory versus when to consult their textbooks, Self-RAG helps AI give you better, more reliable answers by being strategic about when and how to use additional information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt Self RAG](<Self RAG.png>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Components\n",
        "\n",
        "1. **Retrieval Decision**: Determines if retrieval is necessary for a given query.\n",
        "2. **Document Retrieval**: Fetches potentially relevant documents from a vector store.\n",
        "3. **Relevance Evaluation**: Assesses the relevance of retrieved documents to the query.\n",
        "4. **Response Generation**: Generates responses based on relevant contexts.\n",
        "5. **Support Assessment**: Evaluates how well the generated response is supported by the context.\n",
        "6. **Utility Evaluation**: Rates the usefulness of the generated response.\n",
        "\n",
        "## Method Details\n",
        "\n",
        "1. **Retrieval Decision**: The algorithm first decides if retrieval is necessary for the given query. This step prevents unnecessary retrieval for queries that can be answered directly.\n",
        "\n",
        "2. **Document Retrieval**: If retrieval is deemed necessary, the algorithm fetches the top-k most similar documents from a vector store.\n",
        "\n",
        "3. **Relevance Evaluation**: Each retrieved document is evaluated for its relevance to the query. This step filters out irrelevant information, ensuring that only pertinent context is used for generation.\n",
        "\n",
        "4. **Response Generation**: The algorithm generates responses using the relevant contexts. If no relevant contexts are found, it generates a response without retrieval.\n",
        "\n",
        "5. **Support Assessment**: Each generated response is evaluated to determine how well it is supported by the context. This step helps in identifying responses that are grounded in the provided information.\n",
        "\n",
        "6. **Utility Evaluation**: The utility of each response is rated, considering how well it addresses the original query.\n",
        "\n",
        "7. **Response Selection**: The final step involves selecting the best response based on the support assessment and utility evaluation.\n",
        "\n",
        "## Benefits of the Approach\n",
        "\n",
        "1. **Dynamic Retrieval**: By deciding whether retrieval is necessary, the system can adapt to different types of queries efficiently.\n",
        "\n",
        "2. **Relevance Filtering**: The relevance evaluation step ensures that only pertinent information is used, reducing noise in the generation process.\n",
        "\n",
        "3. **Quality Assurance**: The support assessment and utility evaluation provide a way to gauge the quality of generated responses.\n",
        "\n",
        "4. **Flexibility**: The system can generate responses with or without retrieval, adapting to the available information.\n",
        "\n",
        "5. **Improved Accuracy**: By grounding responses in relevant retrieved information and assessing their support, the system can produce more accurate outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-openai pypdf openai chromadb numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# Set up tracking for LangChain (optional - helps you see what's happening behind the scenes)\n",
        "os.environ['LANGSMITH_TRACING'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = '<your-langsmith-api-key>'\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<>your-openai-api-key>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download PDF Documents\n",
        "[MachineLearning-Lecture01](https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf), \n",
        "[MachineLearning-Lecture02](https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture02.pdf), \n",
        "[MachineLearning-Lecture03](https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture03.pdf)\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Update this path to where your PDFs are stored\n",
        "source_path = r\"<your-base-path>\"\n",
        "\n",
        "loaders = [\n",
        "    # First lecture PDF (added twice to simulate real-world messy data)\n",
        "    PyPDFLoader(os.path.join(source_path, \"MachineLearning-Lecture01.pdf\")),\n",
        "    PyPDFLoader(os.path.join(source_path, \"MachineLearning-Lecture02.pdf\")),\n",
        "    PyPDFLoader(os.path.join(source_path, \"MachineLearning-Lecture03.pdf\"))\n",
        "]\n",
        "\n",
        "docs = []\n",
        "\n",
        "# Load each PDF and add its pages to our docs list\n",
        "for i, loader in enumerate(loaders):\n",
        "    print(f\"📄 Loading PDF {i+1}/4...\")\n",
        "    # Each PDF might have multiple pages, so we extend (not append) the list\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,      # Each chunk will be about 1500 characters long\n",
        "    chunk_overlap=150     # Overlap between chunks to maintain context\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "persist_directory = 'docs/chroma/'\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,              # Our document chunks\n",
        "    embedding=embedding,           # The embedding model to convert text to vectors\n",
        "    persist_directory=persist_directory  # Where to save the database\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0) #type: ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining prompt templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RetrievalResponse(BaseModel):\n",
        "    response: str = Field(..., title=\"Determines if retrieval is necessary\", description=\"Output only 'Yes' or 'No'.\")\n",
        "\n",
        "class RelevanceResponse(BaseModel):\n",
        "    response: str = Field(..., title=\"Determines if context is relevant\", description=\"Output only 'Relevant' or 'Irrelevant'.\")\n",
        "\n",
        "class GenerationResponse(BaseModel):\n",
        "    response: str = Field(..., title=\"Generated response\", description=\"The generated response.\")\n",
        "\n",
        "class SupportResponse(BaseModel):\n",
        "    response: str = Field(..., title=\"Determines if response is supported\", description=\"Output 'Fully supported', 'Partially supported', or 'No support'.\")\n",
        "\n",
        "class UtilityResponse(BaseModel):\n",
        "    response: int = Field(..., title=\"Utility rating\", description=\"Rate the utility of the response from 1 to 5.\")\n",
        "\n",
        "# Your prompts remain the same\n",
        "retrieval_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"Given the query '{query}', determine if retrieval is necessary. Output only 'Yes' or 'No'.\"\n",
        ")\n",
        "\n",
        "relevance_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"context\"],\n",
        "    template=\"Given the query '{query}' and the context '{context}', determine if the context is relevant. Output only 'Relevant' or 'Irrelevant'.\"\n",
        ")\n",
        "\n",
        "generation_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"context\"],\n",
        "    template=\"Given the query '{query}' and the context '{context}', generate a response.\"\n",
        ")\n",
        "\n",
        "support_prompt = PromptTemplate(\n",
        "    input_variables=[\"response\", \"context\"],\n",
        "    template=\"Given the response '{response}' and the context '{context}', determine if the response is supported by the context. Output 'Fully supported', 'Partially supported', or 'No support'.\"\n",
        ")\n",
        "\n",
        "utility_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"response\"],\n",
        "    template=\"Given the query '{query}' and the response '{response}', rate the utility of the response from 1 to 5.\"\n",
        ")\n",
        "\n",
        "# Create LLMChains - these should work without issues now\n",
        "retrieval_chain = retrieval_prompt | llm.with_structured_output(RetrievalResponse)\n",
        "relevance_chain = relevance_prompt | llm.with_structured_output(RelevanceResponse)\n",
        "generation_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n",
        "support_chain = support_prompt | llm.with_structured_output(SupportResponse)\n",
        "utility_chain = utility_prompt | llm.with_structured_output(UtilityResponse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the self RAG logic flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def self_rag(query, vectorstore, top_k=3):\n",
        "    print(f\"\\nProcessing query: {query}\")\n",
        "    \n",
        "    # Step 1: Determine if retrieval is necessary\n",
        "    print(\"Step 1: Determining if retrieval is necessary...\")\n",
        "    input_data = {\"query\": query}\n",
        "    retrieval_decision = retrieval_chain.invoke(input_data).response.strip().lower()\n",
        "    print(f\"Retrieval decision: {retrieval_decision}\")\n",
        "    \n",
        "    if retrieval_decision == 'yes':\n",
        "        # Step 2: Retrieve relevant documents\n",
        "        print(\"Step 2: Retrieving relevant documents...\")\n",
        "        docs = vectorstore.similarity_search(query, k=top_k)\n",
        "        contexts = [doc.page_content for doc in docs]\n",
        "        print(f\"Retrieved {len(contexts)} documents\")\n",
        "        \n",
        "        # Step 3: Evaluate relevance of retrieved documents\n",
        "        print(\"Step 3: Evaluating relevance of retrieved documents...\")\n",
        "        relevant_contexts = []\n",
        "        for i, context in enumerate(contexts):\n",
        "            input_data = {\"query\": query, \"context\": context}\n",
        "            relevance = relevance_chain.invoke(input_data).response.strip().lower() #type: ignore\n",
        "            print(f\"Document {i+1} relevance: {relevance}\")\n",
        "            if relevance == 'relevant':\n",
        "                relevant_contexts.append(context)\n",
        "        \n",
        "        print(f\"Number of relevant contexts: {len(relevant_contexts)}\")\n",
        "        \n",
        "        # If no relevant contexts found, generate without retrieval\n",
        "        if not relevant_contexts:\n",
        "            print(\"No relevant contexts found. Generating without retrieval...\")\n",
        "            input_data = {\"query\": query, \"context\": \"No relevant context found.\"}\n",
        "            return generation_chain.invoke(input_data).response #type: ignore\n",
        "        \n",
        "        # Step 4: Generate response using relevant contexts\n",
        "        print(\"Step 4: Generating responses using relevant contexts...\")\n",
        "        responses = []\n",
        "        for i, context in enumerate(relevant_contexts):\n",
        "            print(f\"Generating response for context {i+1}...\")\n",
        "            input_data = {\"query\": query, \"context\": context}\n",
        "            response = generation_chain.invoke(input_data).response #type: ignore\n",
        "            \n",
        "            # Step 5: Assess support\n",
        "            print(f\"Step 5: Assessing support for response {i+1}...\")\n",
        "            input_data = {\"response\": response, \"context\": context}\n",
        "            support = support_chain.invoke(input_data).response.strip().lower()\n",
        "            print(f\"Support assessment: {support}\")\n",
        "            \n",
        "            # Step 6: Evaluate utility\n",
        "            print(f\"Step 6: Evaluating utility for response {i+1}...\")\n",
        "            input_data = {\"query\": query, \"response\": response}\n",
        "            utility = int(utility_chain.invoke(input_data).response) #type: ignore\n",
        "            print(f\"Utility score: {utility}\")\n",
        "            \n",
        "            responses.append((response, support, utility))\n",
        "        \n",
        "        # Select the best response based on support and utility\n",
        "        print(\"Selecting the best response...\")\n",
        "        best_response = max(responses, key=lambda x: (x[1] == 'fully supported', x[2]))\n",
        "        print(f\"Best response support: {best_response[1]}, utility: {best_response[2]}\")\n",
        "        return best_response[0]\n",
        "    else:\n",
        "        # Generate without retrieval\n",
        "        print(\"Generating without retrieval...\")\n",
        "        input_data = {\"query\": query, \"context\": \"No retrieval necessary.\"}\n",
        "        return generation_chain.invoke(input_data).response #type: ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the self-RAG function easy query with high relevance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What are major topics for this class?\"\n",
        "response = self_rag(query, vectorstore)\n",
        "\n",
        "print(\"\\nFinal response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the self-RAG function with a more challenging query with low relevance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What are few main formulas in statistics?\"\n",
        "response = self_rag(query, vectorstore)\n",
        "\n",
        "print(\"\\nFinal response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--self-rag)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
