{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide to Retrieval Methods in LangChain\n",
    "\n",
    "## What is Retrieval?\n",
    "Retrieval is the process of finding the most relevant documents from a large collection based on a user's question. Think of it like a smart search engine that understands meaning, not just keywords.\n",
    "\n",
    "**Why do we need different retrieval methods?**\n",
    "- Different situations require different approaches\n",
    "- Some methods are faster, others are more accurate\n",
    "- Your choice depends on your specific use case\n",
    "\n",
    "In this tutorial, we'll explore various retrieval methods and learn when to use each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Before we start, let's install the required packages and set up our environment.\n",
    "\n",
    "**Why these packages?**\n",
    "- `langchain_community`: Community-contributed components\n",
    "- `langchain`: Core LangChain functionality\n",
    "- `pypdf`: For reading PDF documents\n",
    "- `langchain-openai`: OpenAI integrations (embeddings, LLMs)\n",
    "- `chromadb`: Vector database for storing embeddings\n",
    "- `lark`: Parser for query construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain_community\n",
    "!pip install langchain\n",
    "!pip install pypdf\n",
    "!pip install langchain-openai\n",
    "!pip install chromadb\n",
    "!pip install lark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Environment Variables\n",
    "\n",
    "**Why do we need API keys?**\n",
    "- LangSmith: For tracing and debugging our applications\n",
    "- OpenAI: For embeddings (converting text to vectors) and language models\n",
    "\n",
    "**What are embeddings?**\n",
    "Embeddings are numerical representations of text that capture semantic meaning. Words/sentences with similar meanings have similar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '<your-api-key>'  # Replace with your actual key\n",
    "os.environ['OPENAI_API_KEY'] = '<your-api-key>'     # Replace with your actual key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Vector Database Setup\n",
    "\n",
    "**What is a Vector Database?**\n",
    "A vector database stores documents as vectors (numerical representations) that capture their meaning. This allows us to find similar documents based on semantic similarity, not just keyword matching.\n",
    "\n",
    "**Why Chroma?**\n",
    "- Easy to use and set up\n",
    "- Works well for small to medium datasets\n",
    "- Supports persistence (saves data to disk)\n",
    "- Good for learning and prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Create embeddings object - this converts text to vectors\n",
    "# Think of this as a translator that turns words into numbers\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# Set up the vector database directory\n",
    "# This is where our vectors will be stored on disk\n",
    "persist_directory = 'docs/chroma/'\n",
    "\n",
    "# Create or load existing vector database\n",
    "# If the directory exists, it loads the existing data\n",
    "# If not, it creates a new empty database\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "# Check how many documents are in our database\n",
    "print(f\"Number of documents in database: {vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Similarity Search\n",
    "\n",
    "**What is Similarity Search?**\n",
    "Similarity search finds documents that are most similar to your query. It works by:\n",
    "1. Converting your question to a vector\n",
    "2. Comparing it with all document vectors\n",
    "3. Returning the most similar ones\n",
    "\n",
    "**When to use it?**\n",
    "- When you want quick, straightforward results\n",
    "- For general question-answering\n",
    "- When you don't need diverse results\n",
    "\n",
    "Let's start with a simple example using mushroom data to understand how similarity search works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents about mushrooms\n",
    "# These are our \"knowledge base\" - like having a small encyclopedia\n",
    "texts = [\n",
    "    \"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\",\n",
    "    \"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\",\n",
    "    \"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\",\n",
    "]\n",
    "\n",
    "# Create a small vector database from these texts\n",
    "# This converts our text into vectors and stores them\n",
    "smalldb = Chroma.from_texts(texts, embedding=embedding)\n",
    "\n",
    "# Define our question\n",
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
    "\n",
    "# Perform similarity search - find the 2 most similar documents\n",
    "# k=2 means \"give me the top 2 most similar results\"\n",
    "results = smalldb.similarity_search(question, k=2)\n",
    "\n",
    "print(\"üîç Similarity Search Results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"{doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened?** \n",
    "The search found documents that mention \"large fruiting body\" and \"all-white\" mushrooms, which are most relevant to our question. Notice how it understood the meaning, not just exact word matches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Maximum Marginal Relevance (MMR)\n",
    "\n",
    "**What is the problem with regular similarity search?**\n",
    "Sometimes similarity search returns very similar documents - like getting the same information repeated multiple times. This isn't very helpful!\n",
    "\n",
    "**What is MMR?**\n",
    "Maximum Marginal Relevance (MMR) tries to solve this by:\n",
    "1. Finding relevant documents (like similarity search)\n",
    "2. **AND** ensuring diversity among the results\n",
    "3. Balancing relevance vs. diversity\n",
    "\n",
    "**When to use MMR?**\n",
    "- When you want diverse information\n",
    "- When regular search gives you repetitive results\n",
    "- When you need a comprehensive view of a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MMR to get diverse results\n",
    "# fetch_k=3 means \"look at 3 candidates first\"\n",
    "# k=2 means \"give me 2 final results\"\n",
    "mmr_results = smalldb.max_marginal_relevance_search(question, k=2, fetch_k=3)\n",
    "\n",
    "print(\"üéØ Maximum Marginal Relevance Results:\")\n",
    "for i, doc in enumerate(mmr_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"{doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the difference:** MMR gives us diverse information - one about physical characteristics and another about toxicity. This provides a more complete picture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Regular vs MMR Search\n",
    "\n",
    "Let's see how regular search can return duplicate content while MMR provides variety using real documents.\n",
    "\n",
    "**Why are we loading PDFs?**\n",
    "- Real documents are more complex than our simple mushroom examples\n",
    "- PDFs often contain repetitive information\n",
    "- This makes the difference between similarity search and MMR more obvious\n",
    "\n",
    "**What is text splitting?**\n",
    "Large documents need to be split into smaller chunks because:\n",
    "- Most AI models have token limits\n",
    "- Smaller chunks are easier to match with queries\n",
    "- Better retrieval accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "source_path = r\"<your-source-path>\"  # Replace with your actual path\n",
    "\n",
    "# Create a list of PDF files to load\n",
    "# Note: We're intentionally adding the same PDF twice to show how RAG handles duplicates\n",
    "loaders = [\n",
    "    # First lecture PDF (added twice on purpose to simulate messy real-world data)\n",
    "    PyPDFLoader(rf\"{source_path}/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(rf\"{source_path}/MachineLearning-Lecture01.pdf\"),  # Duplicate!\n",
    "    # Additional lecture PDFs\n",
    "    PyPDFLoader(rf\"{source_path}/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(rf\"{source_path}/MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "\n",
    "# Create an empty list to store all our document pages\n",
    "docs = []\n",
    "\n",
    "# Load each PDF and add its pages to our docs list\n",
    "for loader in loaders:\n",
    "    # Each PDF might have multiple pages, so we extend (not append) the list\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "print(f\"üìÑ Loaded {len(docs)} pages from {len(loaders)} PDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitting Strategy\n",
    "\n",
    "**Why these specific settings?**\n",
    "- `chunk_size=1500`: Each chunk is about 1500 characters (roughly 300-400 words)\n",
    "- `chunk_overlap=150`: 150 characters overlap between chunks to maintain context\n",
    "\n",
    "**What does overlap do?**\n",
    "Overlap ensures that if a sentence or concept spans across chunks, it won't be completely lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter with specific settings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,      # Each chunk will be about 1500 characters long\n",
    "    chunk_overlap=150     # Overlap between chunks to maintain context\n",
    ")\n",
    "\n",
    "# Split all documents into smaller chunks\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"üìÑ Created {len(splits)} chunks from {len(docs)} pages\")\n",
    "\n",
    "# Create a new vector database with our document chunks\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,              # Our document chunks\n",
    "    embedding=embedding,           # The embedding model to convert text to vectors\n",
    "    persist_directory=persist_directory  # Where to save the database\n",
    ")\n",
    "\n",
    "print(f\"üíæ Vector database created with {vectordb._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Search Methods\n",
    "\n",
    "Now let's test both similarity search and MMR on the same question to see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a question about MATLAB\n",
    "question = \"what did they say about matlab?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Regular similarity search\n",
    "docs_ss = vectordb.similarity_search(question, k=3)\n",
    "\n",
    "print(\"üìã Regular Similarity Search - First 100 characters:\")\n",
    "for i, doc in enumerate(docs_ss, 1):\n",
    "    print(f\"Result {i}: {doc.page_content[:100]}...\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# MMR search\n",
    "docs_mmr = vectordb.max_marginal_relevance_search(question, k=3)\n",
    "\n",
    "print(\"üéØ MMR Search - First 100 characters:\")\n",
    "for i, doc in enumerate(docs_mmr, 1):\n",
    "    print(f\"Result {i}: {doc.page_content[:100]}...\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight:** Regular search might return very similar results (especially since we have duplicate PDFs), while MMR provides more diverse information from different sources or contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using Metadata for Specific Searches\n",
    "\n",
    "**What is Metadata?**\n",
    "Metadata is extra information about each document, like:\n",
    "- Source file name\n",
    "- Page number\n",
    "- Creation date\n",
    "- Author\n",
    "\n",
    "**Why use metadata filtering?**\n",
    "- Search within specific documents or sources\n",
    "- Filter by date ranges\n",
    "- Find information from particular authors\n",
    "- Make searches more precise\n",
    "\n",
    "**When to use metadata filtering?**\n",
    "- When you know the source you want to search in\n",
    "- When you need information from a specific time period\n",
    "- When you want to compare information across different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search only in a specific lecture using metadata filter\n",
    "question = \"what did they say about regression in the third lecture?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\nüîç Searching only in Lecture 3...\")\n",
    "\n",
    "# Filter results to only include documents from lecture 3\n",
    "# This is like saying \"only search in this specific file\"\n",
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    # The filter matches documents with this exact source path\n",
    "    filter={\"source\": rf\"{source_path}/MachineLearning-Lecture03.pdf\"}\n",
    ")\n",
    "\n",
    "print(\"üìö Search Results from Lecture 3 Only:\")\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Page: {doc.metadata.get('page', 'Unknown')}\")\n",
    "    print(f\"Content preview: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened?** \n",
    "We filtered results to only show content from the third lecture, making our search more specific. This is useful when you want to find information from a particular source or time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Self-Query Retriever (Smart Filtering)\n",
    "\n",
    "**What's the problem with manual filtering?**\n",
    "In the previous example, we had to manually specify the exact filter. But what if users ask questions like \"What did they say about regression in the third lecture?\" - they don't want to specify technical filters!\n",
    "\n",
    "**What is Self-Query Retriever?**\n",
    "Self-Query Retriever is smart because it:\n",
    "1. Analyzes the user's question\n",
    "2. Automatically determines what filters to apply\n",
    "3. Extracts the actual search query\n",
    "4. Applies filters based on the question's context\n",
    "\n",
    "**When to use Self-Query?**\n",
    "- When users ask questions that include filtering criteria\n",
    "- When you want a more natural search experience\n",
    "- When you have structured metadata that users might reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "# Define what metadata fields are available\n",
    "# This tells the AI what filters it can use\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=f\"The lecture the chunk is from, should be one of `{source_path}/MachineLearning-Lecture01.pdf`, `{source_path}/MachineLearning-Lecture02.pdf`, or `{source_path}/MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Describe what the documents contain\n",
    "# This gives context to the AI about what it's searching through\n",
    "document_content_description = \"Lecture notes from machine learning classes\"\n",
    "\n",
    "# Create the language model for filtering\n",
    "# This AI model will understand the question and create appropriate filters\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
    "\n",
    "# Create the self-query retriever\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True  # Show us what it's doing\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Self-Query Retriever created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Self-Query Retriever\n",
    "\n",
    "Watch how the AI automatically understands \"third lecture\" and applies the appropriate filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the self-query retriever\n",
    "question = \"what did they say about regression in the third lecture?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\nüîç Processing...\")\n",
    "\n",
    "# Get relevant documents - the AI will automatically filter for lecture 3\n",
    "# Watch the verbose output to see how it constructs the query!\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "print(\"\\nüìö Results:\")\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Page: {doc.metadata.get('page', 'Unknown')}\")\n",
    "    print(f\"Content preview: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazing!** The AI automatically understood that \"third lecture\" means we want to filter by the third lecture source file. No manual filter specification needed!\n",
    "\n",
    "View your langsmith dashboard to understand Query calls under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Contextual Compression\n",
    "\n",
    "**What's the problem with long documents?**\n",
    "Sometimes retrieved documents contain a lot of irrelevant information mixed with the relevant parts. This:\n",
    "- Wastes tokens (costs more money)\n",
    "- Makes it harder for the AI to find the important information\n",
    "- Can lead to poorer responses\n",
    "\n",
    "**What is Contextual Compression?**\n",
    "Contextual compression:\n",
    "1. Retrieves documents normally\n",
    "2. Uses an AI model to extract only the relevant parts\n",
    "3. Returns compressed, focused content\n",
    "\n",
    "**When to use compression?**\n",
    "- When your documents are long and contain mixed information\n",
    "- When you want to reduce token usage\n",
    "- When you need more focused, relevant responses\n",
    "- When working with limited context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Function to display results nicely\n",
    "def pretty_print_docs(docs):\n",
    "    separator = \"\\n\" + \"-\" * 100 + \"\\n\"\n",
    "    content = separator.join([f\"Document {i+1}:\\n\\n{d.page_content}\" for i, d in enumerate(docs)])\n",
    "    print(content)\n",
    "\n",
    "# Create the compressor\n",
    "# This AI model will read the full document and extract only relevant parts\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Wrap our vector database with compression\n",
    "# This creates a \"smart\" retriever that compresses results\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,      # The AI that does the compression\n",
    "    base_retriever=vectordb.as_retriever()  # Our regular retriever\n",
    ")\n",
    "\n",
    "print(\"üóúÔ∏è Compression retriever created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Compression\n",
    "\n",
    "Let's see how compression extracts only the relevant parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test compression\n",
    "question = \"what did they say about matlab?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\nüîç Getting compressed results...\")\n",
    "\n",
    "# Get compressed documents\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "\n",
    "print(\"\\nüìÑ Compressed Results:\")\n",
    "pretty_print_docs(compressed_docs)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Compare with uncompressed results\n",
    "print(\"üìÑ Original (Uncompressed) Results for comparison:\")\n",
    "uncompressed_docs = vectordb.similarity_search(question, k=2)\n",
    "for i, doc in enumerate(uncompressed_docs, 1):\n",
    "    print(f\"\\nOriginal Document {i}:\")\n",
    "    print(f\"{doc.page_content[:500]}...\")  # First 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** The compression removed irrelevant text and kept only the parts that answer our question about MATLAB. This makes the results more focused and saves tokens!\n",
    "\n",
    "View your langsmith dashboard to understand Query calls under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Combining Techniques\n",
    "\n",
    "**Why combine techniques?**\n",
    "Different techniques solve different problems:\n",
    "- MMR provides diversity\n",
    "- Compression provides focus\n",
    "- Combined: diverse AND focused results\n",
    "\n",
    "**Best practices for combining:**\n",
    "- Start with the base retrieval method (similarity or MMR)\n",
    "- Add compression to reduce noise\n",
    "- Use filtering when you need specific sources\n",
    "- Test different combinations to see what works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine compression with MMR\n",
    "# This gives us both diverse and concise results\n",
    "compression_retriever_mmr = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type=\"mmr\")  # Use MMR instead of similarity\n",
    ")\n",
    "\n",
    "# Test the combined approach\n",
    "question = \"what did they say about matlab?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\nüîç Getting compressed + MMR results...\")\n",
    "\n",
    "compressed_docs = compression_retriever_mmr.get_relevant_documents(question)\n",
    "\n",
    "print(\"\\nüìÑ Compressed + MMR Results:\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best of both worlds:** We get diverse results (MMR) that are also concise (compression). This is often the best approach for complex queries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Alternative Retrieval Methods\n",
    "\n",
    "**Beyond embeddings:** Not all retrieval needs to use embeddings. Sometimes traditional methods work better!\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
    "- **What it is:** Measures how important a word is to a document\n",
    "- **How it works:** Frequent words in a document but rare overall = important\n",
    "- **When to use:** When you want exact keyword matching\n",
    "- **Pros:** Fast, no need for embeddings, good for specific terms\n",
    "- **Cons:** Misses semantic meaning (\"car\" vs \"automobile\")\n",
    "\n",
    "**SVM (Support Vector Machine):**\n",
    "- **What it is:** Machine learning classification for retrieval\n",
    "- **How it works:** Learns to classify documents as relevant/irrelevant\n",
    "- **When to use:** When you have training data about what's relevant\n",
    "- **Pros:** Good for specific domains, can learn patterns\n",
    "- **Cons:** Requires training data, more complex setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever, TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load a PDF document\n",
    "loader = PyPDFLoader(rf\"{source_path}/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# Combine all pages into one text\n",
    "# This creates a single string from all pages\n",
    "all_page_text = [p.page_content for p in pages]\n",
    "joined_page_text = \" \".join(all_page_text)\n",
    "\n",
    "# Split the text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,    # Each chunk is about 1500 characters\n",
    "    chunk_overlap=150   # 150 characters overlap between chunks\n",
    ")\n",
    "splits = text_splitter.split_text(joined_page_text)\n",
    "\n",
    "print(f\"üìÑ Created {len(splits)} text chunks from the PDF\")\n",
    "\n",
    "# Create different types of retrievers\n",
    "print(\"\\nüîß Creating retrievers...\")\n",
    "\n",
    "# SVM Retriever - uses machine learning classification\n",
    "# Note: This will train a model on your data\n",
    "svm_retriever = SVMRetriever.from_texts(splits, embedding)\n",
    "\n",
    "# TF-IDF Retriever - uses word frequency statistics\n",
    "# Note: This doesn't need embeddings!\n",
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)\n",
    "\n",
    "print(\"‚úÖ All retrievers created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Alternative Retrievers\n",
    "\n",
    "Let's test each retriever to see how they perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVM retriever\n",
    "question = \"What are major topics for this class?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\nü§ñ SVM Retriever Result:\")\n",
    "\n",
    "docs_svm = svm_retriever.get_relevant_documents(question)\n",
    "print(f\"Content preview: {docs_svm[0].page_content[:300]}...\")\n",
    "print(f\"\\nNumber of results: {len(docs_svm)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test TF-IDF retriever\n",
    "question = \"what did they say about matlab?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\nüìä TF-IDF Retriever Result:\")\n",
    "\n",
    "docs_tfidf = tfidf_retriever.get_relevant_documents(question)\n",
    "print(f\"Content preview: {docs_tfidf[0].page_content[:300]}...\")\n",
    "print(f\"\\nNumber of results: {len(docs_tfidf)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Compare with embedding-based search\n",
    "print(\"üîç Embedding-based Similarity Search (for comparison):\")\n",
    "docs_embedding = vectordb.similarity_search(question, k=1)\n",
    "print(f\"Content preview: {docs_embedding[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison insights:**\n",
    "- **TF-IDF:** Likely found exact matches for \"matlab\" keyword\n",
    "- **SVM:** Used machine learning to classify relevance\n",
    "- **Embedding:** Found semantically similar content (might catch variations like \"MATLAB\", \"programming environment\", etc.)\n",
    "\n",
    "Each method has strengths depending on your use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Choosing the Right Method: Decision Guide\n",
    "\n",
    "**How do you choose which method to use?** Here's a practical guide:\n",
    "\n",
    "### Start with these questions:\n",
    "\n",
    "1. **Do you need exact keyword matching?** ‚Üí Use TF-IDF\n",
    "2. **Do you need semantic understanding?** ‚Üí Use embedding-based methods\n",
    "3. **Are results too similar?** ‚Üí Add MMR\n",
    "4. **Are documents too long?** ‚Üí Add compression\n",
    "5. **Do you need source-specific results?** ‚Üí Use metadata filtering or self-query\n",
    "\n",
    "### Method Comparison Table\n",
    "\n",
    "| Method | Best For | Pros | Cons | When to Use |\n",
    "|--------|----------|------|------|-------------|\n",
    "| **Similarity Search** | Quick, simple searches | Fast, easy to understand | May return similar results | General Q&A, simple queries |\n",
    "| **MMR** | Diverse results needed | Reduces redundancy | Slightly slower | When you need comprehensive coverage |\n",
    "| **Metadata Filtering** | Source-specific searches | Very precise | Requires good metadata | Known source constraints |\n",
    "| **Self-Query** | Natural language filtering | User-friendly | Needs more setup | Complex queries with filters |\n",
    "| **Compression** | Long documents | Saves tokens, focused results | Additional processing time | Detailed documents, cost control |\n",
    "| **TF-IDF** | Keyword matching | No embeddings needed | Misses semantic meaning | Exact term searches |\n",
    "| **SVM** | Classification-based | Good for specific domains | Requires training data | Domain-specific applications |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Practical Examples: Common Use Cases\n",
    "\n",
    "Let's look at some real-world scenarios and which methods work best:\n",
    "\n",
    "### Use Case 1: Customer Support Chatbot\n",
    "**Scenario:** Help customers find answers in product documentation\n",
    "**Best approach:** Similarity Search + Compression\n",
    "**Why:** Fast responses, focused answers, handles varied ways of asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Customer support scenario\n",
    "customer_question = \"How do I reset my password?\"\n",
    "\n",
    "print(f\"Customer Question: {customer_question}\")\n",
    "print(\"\\nüéß Customer Support Response:\")\n",
    "\n",
    "# Use compression for focused, helpful answers\n",
    "support_docs = compression_retriever.get_relevant_documents(customer_question)\n",
    "\n",
    "# In a real chatbot, you'd feed this to an LLM for a natural response\n",
    "print(\"Retrieved information:\")\n",
    "for i, doc in enumerate(support_docs[:1], 1):  # Just show first result\n",
    "    print(f\"\\nRelevant content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 2: Research Assistant\n",
    "**Scenario:** Help researchers find diverse information on a topic\n",
    "**Best approach:** MMR + Self-Query\n",
    "**Why:** Diverse results, can handle complex queries with filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Research scenario\n",
    "research_question = \"What are the different approaches to machine learning mentioned in these lectures?\"\n",
    "\n",
    "print(f\"Research Question: {research_question}\")\n",
    "print(\"\\nüî¨ Research Assistant Response:\")\n",
    "\n",
    "# Use MMR for diverse information\n",
    "research_docs = vectordb.max_marginal_relevance_search(research_question, k=3)\n",
    "\n",
    "print(\"Diverse research findings:\")\n",
    "for i, doc in enumerate(research_docs, 1):\n",
    "    print(f\"\\nApproach {i}: {doc.page_content[:150]}...\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 3: Legal Document Search\n",
    "**Scenario:** Find specific legal precedents or clauses\n",
    "**Best approach:** TF-IDF + Metadata Filtering\n",
    "**Why:** Exact term matching important, need source-specific results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Legal scenario (simulated)\n",
    "legal_question = \"regression analysis\"\n",
    "\n",
    "print(f\"Legal Search Term: {legal_question}\")\n",
    "print(\"\\n‚öñÔ∏è Legal Document Search:\")\n",
    "\n",
    "# Use TF-IDF for exact term matching\n",
    "legal_docs = tfidf_retriever.get_relevant_documents(legal_question)\n",
    "\n",
    "print(\"Exact term matches:\")\n",
    "for i, doc in enumerate(legal_docs[:2], 1):\n",
    "    print(f\"\\nMatch {i}: {doc.page_content[:200]}...\")\n",
    "    # In real legal search, you'd also show metadata like case numbers, dates, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance and Cost Considerations\n",
    "\n",
    "**Why does this matter?**\n",
    "Different methods have different costs and speeds. Choose based on your needs:\n",
    "\n",
    "### Speed Ranking (Fastest to Slowest):\n",
    "1. **TF-IDF** - No AI calls needed\n",
    "2. **Similarity Search** - Simple vector comparison\n",
    "3. **MMR** - Additional diversity calculations\n",
    "4. **Self-Query** - Requires LLM call for query construction\n",
    "5. **Compression** - Requires LLM call for each document\n",
    "\n",
    "### Cost Ranking (Cheapest to Most Expensive):\n",
    "1. **TF-IDF** - No API calls\n",
    "2. **Similarity Search** - Only embedding API calls\n",
    "3. **MMR** - Only embedding API calls\n",
    "4. **Self-Query** - Embedding + LLM API calls\n",
    "5. **Compression** - Embedding + LLM API calls for each result\n",
    "\n",
    "### Quality Ranking (for semantic search):\n",
    "1. **Compression + MMR** - Best quality, most focused\n",
    "2. **MMR** - Good quality, diverse results\n",
    "3. **Similarity Search** - Good quality, may have duplicates\n",
    "4. **Self-Query** - Good quality, precise filtering\n",
    "5. **TF-IDF** - Good for keywords, poor for semantic meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Troubleshooting Common Issues\n",
    "\n",
    "### Problem 1: Results are too similar\n",
    "**Solution:** Use MMR instead of similarity search\n",
    "\n",
    "### Problem 2: Results are too long/verbose\n",
    "**Solution:** Add compression to extract only relevant parts\n",
    "\n",
    "### Problem 3: Can't find information from specific sources\n",
    "**Solution:** Use metadata filtering or self-query retriever\n",
    "\n",
    "### Problem 4: Missing obvious keyword matches\n",
    "**Solution:** Try TF-IDF retriever or combine with similarity search\n",
    "\n",
    "### Problem 5: Responses are too slow\n",
    "**Solution:** Use simpler methods (similarity search, TF-IDF) and reduce k value\n",
    "\n",
    "### Problem 6: Costs are too high\n",
    "**Solution:** Use compression to reduce token usage, or switch to TF-IDF for keyword queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Best Practices and Tips\n",
    "\n",
    "### 1. Start Simple, Add Complexity\n",
    "- Begin with basic similarity search\n",
    "- Add MMR if results are too similar\n",
    "- Add compression if documents are too long\n",
    "- Add filtering if you need source-specific results\n",
    "\n",
    "### 2. Optimize Your Chunks\n",
    "- **Chunk size:** 1000-2000 characters usually works well\n",
    "- **Overlap:** 10-20% of chunk size\n",
    "- **Test different sizes** for your specific use case\n",
    "\n",
    "### 3. Use Good Metadata\n",
    "- Include source, date, author, section\n",
    "- Make metadata human-readable\n",
    "- Update metadata when documents change\n",
    "\n",
    "### 4. Test with Real Queries\n",
    "- Use actual user questions, not made-up ones\n",
    "- Test edge cases and typos\n",
    "- Measure success rate, not just technical metrics\n",
    "\n",
    "### 5. Monitor and Iterate\n",
    "- Track which queries work well/poorly\n",
    "- Adjust chunk sizes based on performance\n",
    "- Update retrieval methods as your data changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Quick Reference: Code Templates\n",
    "\n",
    "### Basic Setup Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for basic retrieval setup\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Set up embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# 2. Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Create vector database\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# 4. Basic search\n",
    "results = vectordb.similarity_search(\"your question\", k=3)\n",
    "print(\"This template is ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Retrieval Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for advanced retrieval with compression and MMR\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 1. Create compressor\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# 2. Create compression retriever with MMR\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    ")\n",
    "\n",
    "# 3. Use it\n",
    "results = compression_retriever.get_relevant_documents(\"your question\")\n",
    "print(\"Advanced retrieval template ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary: When to Use Each Method\n",
    "\n",
    "| Method | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Similarity Search** | Quick, simple searches | Fast, easy to understand | May return similar results |\n",
    "| **MMR** | When you need diverse results | Reduces redundancy | Slightly slower |\n",
    "| **Metadata Filtering** | Specific source/time searches | Very precise | Requires good metadata |\n",
    "| **Self-Query** | Natural language filtering | User-friendly | Needs more setup |\n",
    "| **Compression** | When documents are long | Saves tokens, focused results | Additional processing time |\n",
    "| **TF-IDF** | Traditional keyword matching | No embeddings needed | Misses semantic meaning |\n",
    "| **SVM** | Classification-based retrieval | Good for specific domains | Requires training data |\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Start simple** with similarity search\n",
    "2. **Add complexity** only when you need it\n",
    "3. **Choose methods** based on your specific use case\n",
    "4. **Test with real queries** to validate performance\n",
    "5. **Monitor and iterate** to improve over time\n",
    "\n",
    "### Next Steps:\n",
    "1. Try these methods with your own documents\n",
    "2. Experiment with different chunk sizes\n",
    "3. Combine methods for better results\n",
    "4. Build a complete RAG application\n",
    "\n",
    "Remember: The best retrieval method depends on your specific use case, data type, and performance requirements. Start with the basics and evolve your approach as you learn what works best for your application!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
