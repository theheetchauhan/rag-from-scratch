{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ![](https://europe-west1-atp-views-tracker.cloudfunctions.net/working-analytics?notebook=adaptive-retrieval) -->\n",
        "\n",
        "\n",
        "\n",
        "# Adaptive Retrieval-Augmented Generation (RAG) System\n",
        "\n",
        "## Overview\n",
        "\n",
        "This system implements an advanced Retrieval-Augmented Generation (RAG) approach that adapts its retrieval strategy based on the type of query. By leveraging Language Models (LLMs) at various stages, it aims to provide more accurate, relevant, and context-aware responses to user queries.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Traditional RAG systems often use a one-size-fits-all approach to retrieval, which can be suboptimal for different types of queries. Our adaptive system is motivated by the understanding that different types of questions require different retrieval strategies. For example, a factual query might benefit from precise, focused retrieval, while an analytical query might require a broader, more diverse set of information.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **Query Classifier**: Determines the type of query (Factual, Analytical, Opinion, or Contextual).\n",
        "\n",
        "2. **Adaptive Retrieval Strategies**: Four distinct strategies tailored to different query types:\n",
        "   - Factual Strategy\n",
        "   - Analytical Strategy\n",
        "   - Opinion Strategy\n",
        "   - Contextual Strategy\n",
        "\n",
        "3. **LLM Integration**: LLMs are used throughout the process to enhance retrieval and ranking.\n",
        "\n",
        "4. **OpenAI GPT Model**: Generates the final response using the retrieved documents as context.\n",
        "\n",
        "## Method Details\n",
        "\n",
        "### 1. Query Classification\n",
        "\n",
        "The system begins by classifying the user's query into one of four categories:\n",
        "- Factual: Queries seeking specific, verifiable information.\n",
        "- Analytical: Queries requiring comprehensive analysis or explanation.\n",
        "- Opinion: Queries about subjective matters or seeking diverse viewpoints.\n",
        "- Contextual: Queries that depend on user-specific context.\n",
        "\n",
        "### 2. Adaptive Retrieval Strategies\n",
        "\n",
        "Each query type triggers a specific retrieval strategy:\n",
        "\n",
        "#### Factual Strategy\n",
        "- Enhances the original query using an LLM for better precision.\n",
        "- Retrieves documents based on the enhanced query.\n",
        "- Uses an LLM to rank documents by relevance.\n",
        "\n",
        "#### Analytical Strategy\n",
        "- Generates multiple sub-queries using an LLM to cover different aspects of the main query.\n",
        "- Retrieves documents for each sub-query.\n",
        "- Ensures diversity in the final document selection using an LLM.\n",
        "\n",
        "#### Opinion Strategy\n",
        "- Identifies different viewpoints on the topic using an LLM.\n",
        "- Retrieves documents representing each viewpoint.\n",
        "- Uses an LLM to select a diverse range of opinions from the retrieved documents.\n",
        "\n",
        "#### Contextual Strategy\n",
        "- Incorporates user-specific context into the query using an LLM.\n",
        "- Performs retrieval based on the contextualized query.\n",
        "- Ranks documents considering both relevance and user context.\n",
        "\n",
        "### 3. LLM-Enhanced Ranking\n",
        "\n",
        "After retrieval, each strategy uses an LLM to perform a final ranking of the documents. This step ensures that the most relevant and appropriate documents are selected for the next stage.\n",
        "\n",
        "### 4. Response Generation\n",
        "\n",
        "The final set of retrieved documents is passed to an OpenAI GPT model, which generates a response based on the query and the provided context.\n",
        "\n",
        "## Benefits of This Approach\n",
        "\n",
        "1. **Improved Accuracy**: By tailoring the retrieval strategy to the query type, the system can provide more accurate and relevant information.\n",
        "\n",
        "2. **Flexibility**: The system adapts to different types of queries, handling a wide range of user needs.\n",
        "\n",
        "3. **Context-Awareness**: Especially for contextual queries, the system can incorporate user-specific information for more personalized responses.\n",
        "\n",
        "4. **Diverse Perspectives**: For opinion-based queries, the system actively seeks out and presents multiple viewpoints.\n",
        "\n",
        "5. **Comprehensive Analysis**: The analytical strategy ensures a thorough exploration of complex topics.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This adaptive RAG system represents a significant advancement over traditional RAG approaches. By dynamically adjusting its retrieval strategy and leveraging LLMs throughout the process, it aims to provide more accurate, relevant, and nuanced responses to a wide variety of user queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt Adaptive Retrieval](<adaptive retrieval.png>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install Chroma langchain langchain-openai langchain-community pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Required Libraries\n",
        "\n",
        "Before we dive into the code, let's understand what each library does:\n",
        "\n",
        "- **Chroma**: A vector database that stores our documents as embeddings (numerical representations) and helps us find similar documents quickly\n",
        "- **LangChain**: A framework that makes it easier to build applications with language models, providing tools for document processing, prompts, and chains\n",
        "- **OpenAI**: Provides access to OpenAI's language models like GPT-4 for text generation and embeddings\n",
        "- **Pydantic**: Helps us define data structures with validation, ensuring our code handles data correctly\n",
        "\n",
        "Think of these libraries as specialized tools in a toolbox - each one handles a specific part of our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from typing import List\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Set the OpenAI API key environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<your-api-key>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Query Classification?\n",
        "\n",
        "Query classification is like having a smart assistant that can understand what type of question you're asking. Just as you might approach different types of questions differently in real life, our system needs to understand whether you're asking for:\n",
        "\n",
        "- **Factual information**: \"What is the capital of France?\" or \"How many planets are in our solar system?\"\n",
        "- **Analytical insights**: \"How does climate change affect ocean currents?\" or \"What are the economic impacts of remote work?\"\n",
        "- **Multiple opinions**: \"What do experts think about artificial intelligence?\" or \"What are different approaches to education?\"\n",
        "- **Context-specific answers**: \"What's the best investment strategy for someone in their 20s?\" or \"How should I prepare for a job interview in tech?\"\n",
        "\n",
        "By classifying queries first, our system can choose the most appropriate retrieval strategy for each type of question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the query classifer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class categories_options(BaseModel):\n",
        "        category: str = Field(description=\"The category of the query, the options are: Factual, Analytical, Opinion, or Contextual\", example=\"Factual\")\n",
        "\n",
        "\n",
        "class QueryClassifier:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000) #type: ignore\n",
        "        self.prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Classify the following query into one of these categories: Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nCategory:\"\n",
        "        )\n",
        "        self.chain = self.prompt | self.llm.with_structured_output(categories_options)\n",
        "\n",
        "\n",
        "    def classify(self, query):\n",
        "        return self.chain.invoke(query).category #type: ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How the Query Classifier Works\n",
        "\n",
        "The QueryClassifier is like having a librarian who can instantly categorize any question you ask. Here's what happens:\n",
        "\n",
        "1. **Input**: You give it a question like \"What causes earthquakes?\"\n",
        "2. **Processing**: The classifier uses GPT-4 to analyze the question and determine its type\n",
        "3. **Output**: It returns one of four categories: Factual, Analytical, Opinion, or Contextual\n",
        "\n",
        "The `categories_options` class ensures the classifier always returns a valid category name, preventing errors in our system.\n",
        "\n",
        "**Example classifications:**\n",
        "- \"What is the population of Tokyo?\" → Factual\n",
        "- \"How do social media algorithms affect user behavior?\" → Analytical\n",
        "- \"What do researchers think about the future of renewable energy?\" → Opinion\n",
        "- \"What programming language should I learn as a beginner?\" → Contextual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Retrieval Strategies\n",
        "\n",
        "Think of retrieval strategies as different ways of searching through a library. Just as you might use different approaches to find information depending on what you need, our system uses different strategies for different types of questions.\n",
        "\n",
        "The BaseRetrievalStrategy is like the foundation of a building - it provides the basic tools that all other strategies will use:\n",
        "- **Text splitting**: Breaks down long documents into manageable chunks\n",
        "- **Embeddings**: Converts text into numbers that computers can understand and compare\n",
        "- **Vector database**: Stores and searches through these numerical representations efficiently\n",
        "- **Language model**: Provides intelligent processing capabilities\n",
        "\n",
        "All our specialized strategies will inherit these basic capabilities and add their own unique approaches on top."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Base Retriever class, such that the complex ones will inherit from it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseRetrievalStrategy:\n",
        "    def __init__(self, texts):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
        "        self.documents = text_splitter.create_documents(texts)\n",
        "        self.db = Chroma.from_documents(self.documents, embedding=self.embeddings, persist_directory=None)  # set to \"./chroma_db\" if you want persistence\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000) #type: ignore\n",
        "\n",
        "\n",
        "    def retrieve(self, query, k=4):\n",
        "        return self.db.similarity_search(query, k=k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Factual Retrieval Strategy\n",
        "\n",
        "When someone asks a factual question like \"What is the boiling point of water?\", they want a precise, accurate answer. The Factual strategy is designed specifically for these types of queries.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1. **Query Enhancement**: The system first improves the original question to make it more specific. For example, \"boiling point water\" might become \"What is the boiling point of water at standard atmospheric pressure?\"\n",
        "\n",
        "2. **Document Retrieval**: It searches for documents using the enhanced query, getting more results than needed initially\n",
        "\n",
        "3. **Intelligent Ranking**: Each retrieved document gets a relevance score from 1-10, helping identify the most accurate sources\n",
        "\n",
        "4. **Best Results**: Finally, it returns only the top-ranked documents that are most likely to contain the correct factual information\n",
        "\n",
        "This approach ensures that factual questions get precise, well-sourced answers rather than vague or tangentially related information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Factual retriever strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class relevant_score(BaseModel):\n",
        "        score: float = Field(description=\"The relevance score of the document to the query\", example=8.0)\n",
        "\n",
        "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=4):\n",
        "        print(\"retrieving factual\")\n",
        "        # Use LLM to enhance the query\n",
        "        enhanced_query_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Enhance this factual query for better information retrieval: {query}\"\n",
        "        )\n",
        "        query_chain = enhanced_query_prompt | self.llm\n",
        "        enhanced_query = query_chain.invoke(query).content\n",
        "        print(f'enhande query: {enhanced_query}')\n",
        "\n",
        "        # Retrieve documents using the enhanced query\n",
        "        docs = self.db.similarity_search(enhanced_query, k=k*2) #type: ignore\n",
        "\n",
        "        # Use LLM to rank the relevance of retrieved documents\n",
        "        ranking_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"doc\"],\n",
        "            template=\"On a scale of 1-10, how relevant is this document to the query: '{query}'?\\nDocument: {doc}\\nRelevance score:\"\n",
        "        )\n",
        "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
        "\n",
        "        ranked_docs = []\n",
        "        print(\"ranking docs\")\n",
        "        for doc in docs:\n",
        "            input_data = {\"query\": enhanced_query, \"doc\": doc.page_content}\n",
        "            score = float(ranking_chain.invoke(input_data).score) #type: ignore\n",
        "            ranked_docs.append((doc, score))\n",
        "\n",
        "        # Sort by relevance score and return top k\n",
        "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "        return [doc for doc, _ in ranked_docs[:k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Analytical Retrieval Strategy\n",
        "\n",
        "Analytical questions require a comprehensive understanding of a topic from multiple angles. Think of questions like \"How does artificial intelligence impact the job market?\" - this isn't just asking for one fact, but needs analysis from various perspectives.\n",
        "\n",
        "The Analytical strategy works like a research team:\n",
        "\n",
        "1. **Breaking Down the Question**: It takes your complex question and creates several focused sub-questions. For our AI example, it might generate:\n",
        "   - \"Which jobs are most at risk from AI automation?\"\n",
        "   - \"What new job categories are being created by AI?\"\n",
        "   - \"How are wages affected in AI-impacted industries?\"\n",
        "   - \"What skills are becoming more valuable due to AI?\"\n",
        "\n",
        "2. **Comprehensive Search**: Each sub-question is used to find relevant documents, ensuring broad coverage\n",
        "\n",
        "3. **Diversity Selection**: The system then intelligently selects the most diverse and relevant documents, avoiding repetitive information\n",
        "\n",
        "This approach ensures that analytical questions receive well-rounded, thorough responses that cover multiple aspects of the topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Analytical reriever strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelectedIndices(BaseModel):\n",
        "    indices: List[int] = Field(description=\"Indices of selected documents\", example=[0, 1, 2, 3])\n",
        "\n",
        "class SubQueries(BaseModel):\n",
        "    sub_queries: List[str] = Field(description=\"List of sub-queries for comprehensive analysis\", example=[\"What is the population of New York?\", \"What is the GDP of New York?\"])\n",
        "\n",
        "class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=4):\n",
        "        print(\"retrieving analytical\")\n",
        "        # Use LLM to generate sub-queries for comprehensive analysis\n",
        "        sub_queries_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"k\"],\n",
        "            template=\"Generate {k} sub-questions for: {query}\"\n",
        "        )\n",
        "\n",
        "        llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000) #type: ignore\n",
        "        sub_queries_chain = sub_queries_prompt | llm.with_structured_output(SubQueries)\n",
        "\n",
        "        input_data = {\"query\": query, \"k\": k}\n",
        "        sub_queries = sub_queries_chain.invoke(input_data).sub_queries #type: ignore\n",
        "        print(f'sub queries for comprehensive analysis: {sub_queries}')\n",
        "\n",
        "        all_docs = []\n",
        "        for sub_query in sub_queries:\n",
        "            all_docs.extend(self.db.similarity_search(sub_query, k=2))\n",
        "\n",
        "        # Use LLM to ensure diversity and relevance\n",
        "        diversity_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"docs\", \"k\"],\n",
        "            template=\"\"\"Select the most diverse and relevant set of {k} documents for the query: '{query}'\\nDocuments: {docs}\\n\n",
        "            Return only the indices of selected documents as a list of integers.\"\"\"\n",
        "        )\n",
        "        diversity_chain = diversity_prompt | self.llm.with_structured_output(SelectedIndices)\n",
        "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:50]}...\" for i, doc in enumerate(all_docs)])\n",
        "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
        "        selected_indices_result = diversity_chain.invoke(input_data).indices #type: ignore\n",
        "        print(f'selected diverse and relevant documents')\n",
        "        \n",
        "        return [all_docs[i] for i in selected_indices_result if i < len(all_docs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Opinion Retrieval Strategy\n",
        "\n",
        "Some questions don't have a single \"correct\" answer but instead benefit from multiple perspectives. Questions like \"What do experts think about cryptocurrency?\" or \"How should we address climate change?\" require understanding different viewpoints.\n",
        "\n",
        "The Opinion strategy acts like a balanced journalist:\n",
        "\n",
        "1. **Identifying Viewpoints**: First, it identifies the main perspectives that exist on the topic. For cryptocurrency, it might identify:\n",
        "   - \"Cryptocurrency as revolutionary financial technology\"\n",
        "   - \"Cryptocurrency as speculative bubble\"\n",
        "   - \"Cryptocurrency as regulatory challenge\"\n",
        "\n",
        "2. **Searching Each Perspective**: It then searches for documents that represent each viewpoint\n",
        "\n",
        "3. **Balanced Selection**: Finally, it selects documents that provide a diverse range of opinions, ensuring no single viewpoint dominates\n",
        "\n",
        "This approach helps users understand complex, debated topics by presenting multiple legitimate perspectives rather than a single biased view."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Opinion retriever strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OpinionRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=3):\n",
        "        print(\"retrieving opinion\")\n",
        "        # Use LLM to identify potential viewpoints\n",
        "        viewpoints_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"k\"],\n",
        "            template=\"Identify {k} distinct viewpoints or perspectives on the topic: {query}\"\n",
        "        )\n",
        "        viewpoints_chain = viewpoints_prompt | self.llm\n",
        "        input_data = {\"query\": query, \"k\": k}\n",
        "        viewpoints = viewpoints_chain.invoke(input_data).content.split('\\n') #type: ignore\n",
        "        print(f'viewpoints: {viewpoints}')\n",
        "\n",
        "        all_docs = []\n",
        "        for viewpoint in viewpoints:\n",
        "            all_docs.extend(self.db.similarity_search(f\"{query} {viewpoint}\", k=2))\n",
        "\n",
        "        # Use LLM to classify and select diverse opinions\n",
        "        opinion_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"docs\", \"k\"],\n",
        "            template=\"Classify these documents into distinct opinions on '{query}' and select the {k} most representative and diverse viewpoints:\\nDocuments: {docs}\\nSelected indices:\"\n",
        "        )\n",
        "        opinion_chain = opinion_prompt | self.llm.with_structured_output(SelectedIndices)\n",
        "        \n",
        "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:100]}...\" for i, doc in enumerate(all_docs)])\n",
        "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
        "        selected_indices = opinion_chain.invoke(input_data).indices #type: ignore\n",
        "        print(f'selected diverse and relevant documents')\n",
        "        \n",
        "        return [all_docs[int(i)] for i in selected_indices.split() if i.isdigit() and int(i) < len(all_docs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Contextual Retrieval Strategy\n",
        "\n",
        "Some questions only make sense when we consider the specific situation or context of the person asking. For example, \"What's the best programming language to learn?\" depends heavily on the person's background, goals, and current situation.\n",
        "\n",
        "The Contextual strategy works like a personalized consultant:\n",
        "\n",
        "1. **Understanding Context**: It takes into account any provided user context (like \"I'm a beginner interested in web development\" or \"I'm a data scientist looking to expand my skills\")\n",
        "\n",
        "2. **Reformulating the Query**: It rewrites the question to include this context, making it more specific and targeted\n",
        "\n",
        "3. **Context-Aware Search**: It searches for documents using the contextualized query\n",
        "\n",
        "4. **Contextual Ranking**: When ranking results, it considers both relevance to the topic and appropriateness for the user's specific situation\n",
        "\n",
        "This ensures that context-dependent questions receive personalized, relevant answers rather than generic responses that might not be helpful for the specific user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Contextual retriever strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContextualRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=4, user_context=None):\n",
        "        print(\"retrieving contextual\")\n",
        "        # Use LLM to incorporate user context into the query\n",
        "        context_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"context\"],\n",
        "            template=\"Given the user context: {context}\\nReformulate the query to best address the user's needs: {query}\"\n",
        "        )\n",
        "        context_chain = context_prompt | self.llm\n",
        "        input_data = {\"query\": query, \"context\": user_context or \"No specific context provided\"}\n",
        "        contextualized_query = context_chain.invoke(input_data).content\n",
        "        print(f'contextualized query: {contextualized_query}')\n",
        "\n",
        "        # Retrieve documents using the contextualized query\n",
        "        docs = self.db.similarity_search(contextualized_query, k=k*2) #type: ignore\n",
        "\n",
        "        # Use LLM to rank the relevance of retrieved documents considering the user context\n",
        "        ranking_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"context\", \"doc\"],\n",
        "            template=\"Given the query: '{query}' and user context: '{context}', rate the relevance of this document on a scale of 1-10:\\nDocument: {doc}\\nRelevance score:\"\n",
        "        )\n",
        "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
        "        print(\"ranking docs\")\n",
        "\n",
        "        ranked_docs = []\n",
        "        for doc in docs:\n",
        "            input_data = {\"query\": contextualized_query, \"context\": user_context or \"No specific context provided\", \"doc\": doc.page_content}\n",
        "            score = float(ranking_chain.invoke(input_data).score) #type: ignore\n",
        "            ranked_docs.append((doc, score))\n",
        "\n",
        "\n",
        "        # Sort by relevance score and return top k\n",
        "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return [doc for doc, _ in ranked_docs[:k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bringing It All Together: The Adaptive Retriever\n",
        "\n",
        "Now we have our four specialized strategies, but we need a way to automatically choose which one to use for each question. The AdaptiveRetriever is like a smart dispatcher that:\n",
        "\n",
        "1. **Receives any question** from a user\n",
        "2. **Classifies the question** using our QueryClassifier\n",
        "3. **Routes the question** to the appropriate strategy (Factual, Analytical, Opinion, or Contextual)\n",
        "4. **Returns the results** from the chosen strategy\n",
        "\n",
        "This means users don't need to worry about what type of question they're asking - the system automatically adapts to provide the best possible search approach for each query type.\n",
        "\n",
        "**Example workflow:**\n",
        "- User asks: \"What is the capital of Japan?\"\n",
        "- System classifies: \"Factual\"\n",
        "- System uses: FactualRetrievalStrategy\n",
        "- Result: Precise, well-sourced answer about Tokyo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Adapive retriever class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdaptiveRetriever:\n",
        "    def __init__(self, texts: List[str]):\n",
        "        self.classifier = QueryClassifier()\n",
        "        self.strategies = {\n",
        "            \"Factual\": FactualRetrievalStrategy(texts),\n",
        "            \"Analytical\": AnalyticalRetrievalStrategy(texts),\n",
        "            \"Opinion\": OpinionRetrievalStrategy(texts),\n",
        "            \"Contextual\": ContextualRetrievalStrategy(texts)\n",
        "        }\n",
        "\n",
        "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        category = self.classifier.classify(query)\n",
        "        print(f\"Query classified as: {category}\")\n",
        "        \n",
        "        strategy = self.strategies[category]\n",
        "        print(f\"Using strategy: {strategy}\")\n",
        "        return strategy.retrieve(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making It Compatible with LangChain\n",
        "\n",
        "LangChain has its own standard format for retrievers, so we need to create a wrapper that makes our AdaptiveRetriever compatible with LangChain's ecosystem. The PydanticAdaptiveRetriever is like a translator that allows our custom retriever to work seamlessly with other LangChain components.\n",
        "\n",
        "This wrapper ensures that our adaptive retriever can be easily integrated into larger LangChain applications and follows the framework's conventions for data handling and method signatures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define aditional retriever that inherits from langchain BaseRetriever "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PydanticAdaptiveRetriever(BaseRetriever):\n",
        "    adaptive_retriever: AdaptiveRetriever = Field(exclude=True)\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]: #type: ignore\n",
        "        return self.adaptive_retriever.get_relevant_documents(query)\n",
        "\n",
        "    async def _aget_relevant_documents(self, query: str) -> List[Document]: #type: ignore\n",
        "        return self.get_relevant_documents(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Complete Adaptive RAG System\n",
        "\n",
        "Finally, we bring everything together into a complete Retrieval-Augmented Generation system. The AdaptiveRAG class combines:\n",
        "\n",
        "- **Smart Retrieval**: Our adaptive retriever that chooses the best strategy for each question\n",
        "- **Response Generation**: A powerful language model (GPT-4) that creates natural, helpful responses\n",
        "- **Prompt Engineering**: A carefully crafted prompt that guides the language model to use the retrieved information effectively\n",
        "\n",
        "**How it works end-to-end:**\n",
        "1. **User asks a question**: \"How does exercise affect mental health?\"\n",
        "2. **System classifies**: \"Analytical\" (requires comprehensive understanding)\n",
        "3. **Adaptive retrieval**: Uses analytical strategy to find diverse, relevant documents\n",
        "4. **Response generation**: GPT-4 uses the retrieved documents to create a comprehensive, well-sourced answer\n",
        "5. **User receives**: A detailed response that covers multiple aspects of how exercise affects mental health\n",
        "\n",
        "This creates a system that's both intelligent in how it searches for information and sophisticated in how it presents that information to users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Adaptive RAG class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdaptiveRAG:\n",
        "    def __init__(self, texts: List[str]):\n",
        "        adaptive_retriever = AdaptiveRetriever(texts)\n",
        "        self.retriever = PydanticAdaptiveRetriever(adaptive_retriever=adaptive_retriever)\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000) #type: ignore\n",
        "        \n",
        "        # Create a custom prompt\n",
        "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
        "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "        Answer:\"\"\"\n",
        "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "        \n",
        "        # Create the LLM chain\n",
        "        self.llm_chain = prompt | self.llm\n",
        "        \n",
        "      \n",
        "\n",
        "    def answer(self, query: str) -> str:\n",
        "        docs = self.retriever.get_relevant_documents(query)\n",
        "        input_data = {\"context\": \"\\n\".join([doc.page_content for doc in docs]), \"question\": query}\n",
        "        return self.llm_chain.invoke(input_data) #type: ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Our Test Data\n",
        "\n",
        "Before we can test our adaptive RAG system, we need some sample documents to search through. For this demonstration, we're using a set of texts about Earth and space that represent different types of information:\n",
        "\n",
        "- **Factual statements**: Basic facts that can answer direct questions\n",
        "- **Analytical content**: Information that helps explain relationships and causes\n",
        "- **Opinion/theoretical content**: Different scientific theories and perspectives\n",
        "- **Contextual information**: Broader concepts that depend on understanding relationships\n",
        "\n",
        "In a real-world application, you would replace these sample texts with your own documents - perhaps research papers, product manuals, company knowledge bases, or any other collection of texts you want to make searchable and queryable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demonstrate use of this model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts = [\n",
        "    # Factual content\n",
        "    \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\",\n",
        "    \"The average distance between the Earth and the Sun is about 149.6 million kilometers (93 million miles).\",\n",
        "    \"Earth has one natural satellite, the Moon, which influences tides and stabilizes its axial tilt.\",\n",
        "    \n",
        "    # Analytical context\n",
        "    \"The Earth's distance from the Sun, combined with its atmosphere, allows liquid water to exist, creating conditions suitable for life.\",\n",
        "    \"If Earth were significantly closer to or farther from the Sun, its climate would be too hot or too cold for most known life forms.\",\n",
        "    \n",
        "    # Opinion / theories\n",
        "    \"Theories about the origin of life on Earth include primordial soup theory, hydrothermal vent hypothesis, and panspermia, where life may have originated from space.\",\n",
        "    \n",
        "    # Contextual / broader perspective\n",
        "    \"The Earth's position in the Solar System, within the habitable zone or 'Goldilocks Zone', is a key factor in making it suitable for life.\",\n",
        "    \"The interplay of distance from the Sun, atmospheric composition, and magnetic field contributes to Earth's long-term habitability.\"\n",
        "]\n",
        "rag_system = AdaptiveRAG(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Our Adaptive RAG System\n",
        "\n",
        "Now comes the exciting part - testing our system with different types of questions to see how it adapts its approach. We'll ask four different questions that should trigger each of our four retrieval strategies:\n",
        "\n",
        "1. **Factual Question**: \"What is the distance between the Earth and the Sun?\" - Should trigger the Factual strategy for a precise answer\n",
        "\n",
        "2. **Analytical Question**: \"How does the Earth's distance from the Sun affect its climate?\" - Should trigger the Analytical strategy for comprehensive analysis\n",
        "\n",
        "3. **Opinion Question**: \"What are the different theories about the origin of life on Earth?\" - Should trigger the Opinion strategy to present multiple perspectives\n",
        "\n",
        "4. **Contextual Question**: \"How does the Earth's position in the Solar System influence its habitability?\" - Should trigger the Contextual strategy for situation-dependent understanding\n",
        "\n",
        "Watch the output to see how the system classifies each question and which strategy it chooses. This demonstrates the adaptive nature of our RAG system in action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Showcase the four different types of queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "factual_result = rag_system.answer(\"What is the distance between the Earth and the Sun?\").content #type: ignore\n",
        "print(f\"Answer: {factual_result}\")\n",
        "\n",
        "analytical_result = rag_system.answer(\"How does the Earth's distance from the Sun affect its climate?\").content #type: ignore\n",
        "print(f\"Answer: {analytical_result}\")\n",
        "\n",
        "opinion_result = rag_system.answer(\"What are the different theories about the origin of life on Earth?\").content #type: ignore\n",
        "print(f\"Answer: {opinion_result}\")\n",
        "\n",
        "contextual_result = rag_system.answer(\"How does the Earth's position in the Solar System influence its habitability?\").content #type: ignore\n",
        "print(f\"Answer: {contextual_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What You've Just Learned\n",
        "\n",
        "Congratulations! You've just built and tested a sophisticated Adaptive RAG system. Here's what makes this system special:\n",
        "\n",
        "**Intelligence in Retrieval**: Unlike traditional RAG systems that use the same approach for every question, your system intelligently adapts its search strategy based on what type of question is being asked.\n",
        "\n",
        "**Real-World Applicability**: The four strategies (Factual, Analytical, Opinion, Contextual) cover the vast majority of question types you'll encounter in real applications.\n",
        "\n",
        "**Scalability**: You can easily replace the sample texts with your own documents - whether they're product manuals, research papers, company knowledge bases, or any other text collection.\n",
        "\n",
        "**Extensibility**: The modular design means you can easily add new retrieval strategies, modify existing ones, or integrate additional features like user personalization or domain-specific enhancements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--adaptive-retrieval)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
